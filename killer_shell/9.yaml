apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, nvidia.com/gpu request
        for container alertmanager; cpu, memory, nvidia.com/gpu limit for container
        alertmanager; nvidia.com/gpu request for container config-reloader; nvidia.com/gpu
        limit for container config-reloader'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T15:39:46Z"
    generateName: alertmanager-kps-alertmanager-
    labels:
      alertmanager: kps-alertmanager
      app.kubernetes.io/instance: kps-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.23.0
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      controller-revision-hash: alertmanager-kps-alertmanager-558585597c
      statefulset.kubernetes.io/pod-name: alertmanager-kps-alertmanager-0
    name: alertmanager-kps-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-kps-alertmanager
      uid: 81b7d97a-f9b4-4e1c-afb5-5261e58845d8
    resourceVersion: "368055916"
    uid: 17f7ff92-d04a-4e91-83a2-81fbde60393b
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config/alertmanager.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://kps-alertmanager.monitoring:9093
      - --web.route-prefix=/
      - --cluster.peer=alertmanager-kps-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.23.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 200Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-kps-alertmanager-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fklq5
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fklq5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-kps-alertmanager-0
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: kps-alertmanager
    serviceAccountName: kps-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-kps-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-kps-alertmanager-tls-assets-0
    - emptyDir: {}
      name: alertmanager-kps-alertmanager-db
    - name: kube-api-access-fklq5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:41:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:41:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://962a60a2d11744f9bd8cb3ae777affed3d6c505ddb67c4a63cbe5cb3de5351c2
      image: quay.io/prometheus/alertmanager:v0.23.0
      imageID: quay.io/prometheus/alertmanager@sha256:9ab73a421b65b80be072f96a88df756fc5b52a1bc8d983537b8ec5be8b624c5a
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:40:57Z"
    - containerID: containerd://b34f0cc4bcfdf4216d7dd438f37edc90c532240cc56463152aa9b9963f1a8579
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:40:57Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.206
    podIPs:
    - ip: 10.244.36.206
    qosClass: Burstable
    startTime: "2023-03-24T15:39:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: alertmanager
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, nvidia.com/gpu request
        for container alertmanager; cpu, memory, nvidia.com/gpu limit for container
        alertmanager; nvidia.com/gpu request for container config-reloader; nvidia.com/gpu
        limit for container config-reloader'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T13:21:13Z"
    generateName: alertmanager-sia-dev-kube-prometheus-st-alertmanager-
    labels:
      alertmanager: sia-dev-kube-prometheus-st-alertmanager
      app.kubernetes.io/instance: sia-dev-kube-prometheus-st-alertmanager
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: alertmanager
      app.kubernetes.io/version: 0.23.0
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      controller-revision-hash: alertmanager-sia-dev-kube-prometheus-st-alertmanager-dcb4b554b
      statefulset.kubernetes.io/pod-name: alertmanager-sia-dev-kube-prometheus-st-alertmanager-0
    name: alertmanager-sia-dev-kube-prometheus-st-alertmanager-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: alertmanager-sia-dev-kube-prometheus-st-alertmanager
      uid: 59c0c210-a3a6-4fd2-b2e0-a97ba7384b34
    resourceVersion: "367945861"
    uid: 7c9c5d2a-e126-45ac-8280-e5c88a86cc99
  spec:
    containers:
    - args:
      - --config.file=/etc/alertmanager/config/alertmanager.yaml
      - --storage.path=/alertmanager
      - --data.retention=120h
      - --cluster.listen-address=
      - --web.listen-address=:9093
      - --web.external-url=http://dev-alertmanager.sia-service.kr/
      - --web.route-prefix=/
      - --cluster.peer=alertmanager-sia-dev-kube-prometheus-st-alertmanager-0.alertmanager-operated:9094
      - --cluster.reconnect-timeout=5m
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/prometheus/alertmanager:v0.23.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/healthy
          port: http-web
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: alertmanager
      ports:
      - containerPort: 9093
        name: http-web
        protocol: TCP
      - containerPort: 9094
        name: mesh-tcp
        protocol: TCP
      - containerPort: 9094
        name: mesh-udp
        protocol: UDP
      readinessProbe:
        failureThreshold: 10
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 200Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
      - mountPath: /etc/alertmanager/certs
        name: tls-assets
        readOnly: true
      - mountPath: /alertmanager
        name: alertmanager-sia-dev-kube-prometheus-st-alertmanager-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcs9s
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9093/-/reload
      - --watched-dir=/etc/alertmanager/config
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "-1"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/alertmanager/config
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-pcs9s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: alertmanager-sia-dev-kube-prometheus-st-alertmanager-0
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: sia-dev-kube-prometheus-st-alertmanager
    serviceAccountName: sia-dev-kube-prometheus-st-alertmanager
    subdomain: alertmanager-operated
    terminationGracePeriodSeconds: 120
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: config-volume
      secret:
        defaultMode: 420
        secretName: alertmanager-sia-dev-kube-prometheus-st-alertmanager-generated
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: alertmanager-sia-dev-kube-prometheus-st-alertmanager-tls-assets-0
    - emptyDir: {}
      name: alertmanager-sia-dev-kube-prometheus-st-alertmanager-db
    - name: kube-api-access-pcs9s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6d9f4bad274901c86c0c349a04a2df35b1e2624e5c25ebbe0fccaac529af091c
      image: quay.io/prometheus/alertmanager:v0.23.0
      imageID: quay.io/prometheus/alertmanager@sha256:9ab73a421b65b80be072f96a88df756fc5b52a1bc8d983537b8ec5be8b624c5a
      lastState: {}
      name: alertmanager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T13:21:55Z"
    - containerID: containerd://a001077c1740f6e8fb953d8d5f16e9de472d29026fb6f7e981265952a9cda7a7
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T13:21:55Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.179
    podIPs:
    - ip: 10.244.13.179
    qosClass: Burstable
    startTime: "2023-03-24T13:21:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-04-13T04:41:47Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-48v9q
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "385406332"
    uid: bed87ac7-823a-42ac-a6aa-74eb852fce96
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-102
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-nkf5t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g2080-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-nkf5t
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:41:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:42:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:42:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:41:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://40d038f6a7604c6b4e41fb13c958139d404c9ddcac5393da814836922aed8375
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:41:49Z"
    hostIP: 10.0.0.25
    phase: Running
    podIP: 10.244.33.199
    podIPs:
    - ip: 10.244.33.199
    qosClass: Burstable
    startTime: "2023-04-13T04:41:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:18:11Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-5hmtc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "421513213"
    uid: 528b54cc-393c-4c0b-abf4-5f8d57a2d510
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-202
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-lpcrn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-lpcrn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:18:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:08:19Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:08:19Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:18:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://84cb292e52bcd363595d746acfd9a9a9499541cd3c4bfc90f50f536121f7f815
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState:
        terminated:
          containerID: containerd://854fb600fdc19121f1bdc2b6689124a90f5f7566f4b2ddb56c225f6d21bcfae9
          exitCode: 255
          finishedAt: "2023-05-24T05:07:14Z"
          reason: Unknown
          startedAt: "2023-05-24T03:26:37Z"
      name: exporter
      ready: true
      restartCount: 13
      started: true
      state:
        running:
          startedAt: "2023-05-24T05:07:30Z"
    hostIP: 10.0.0.20
    phase: Running
    podIP: 10.244.30.221
    podIPs:
    - ip: 10.244.30.221
    qosClass: Burstable
    startTime: "2023-03-24T16:18:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-05-17T04:21:01Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-75z76
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "415625178"
    uid: 94bfe36e-b088-4f6e-862f-54a8ef486b6d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-201
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tqktf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-201
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-tqktf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T04:21:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:14:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:14:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T04:21:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8354f5246598de938564c844e419bc245c8ebf320c1370c01a899190264843d3
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState:
        terminated:
          containerID: containerd://647740e4c966f33a4edcf4ff80c0713c518c0baede5b25d30b8963108600f281
          exitCode: 255
          finishedAt: "2023-05-17T10:13:03Z"
          reason: Unknown
          startedAt: "2023-05-17T04:21:03Z"
      name: exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-05-17T10:13:17Z"
    hostIP: 10.0.0.19
    phase: Running
    podIP: 10.244.29.45
    podIPs:
    - ip: 10.244.29.45
    qosClass: Burstable
    startTime: "2023-05-17T04:21:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:15:33Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-8sjkq
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368084616"
    uid: a82aeb73-b941-4853-a0a7-41086ad68103
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-102
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jjz4j
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-jjz4j
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:15:33Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:16:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:16:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:15:33Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3ab6f8e037c7c7ab58629e6aa140a3f5758cfb576c424e2141c961400c3680f6
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:15:34Z"
    hostIP: 10.0.0.9
    phase: Running
    podIP: 10.244.19.56
    podIPs:
    - ip: 10.244.19.56
    qosClass: Burstable
    startTime: "2023-03-24T16:15:33Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-04-13T04:50:49Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-9tqc2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "385412297"
    uid: 33e37b54-65c8-4415-826a-7c137eb255da
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-202
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2gh8b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g2080-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-2gh8b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:50:50Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:51:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:51:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:50:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e6af6dc8a4b2a9f1b1eabf4a633b995ac16bace979be1ddda12805d6bc82c7b1
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:50:52Z"
    hostIP: 10.0.0.26
    phase: Running
    podIP: 10.244.32.10
    podIPs:
    - ip: 10.244.32.10
    qosClass: Burstable
    startTime: "2023-04-13T04:50:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:17:19Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-f9tmw
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368085953"
    uid: 7899ffd2-55d8-49c5-aa92-91947625028a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a6000-101
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g2rxl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-g2rxl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:17:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:18:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:18:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:17:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://08c4577bd0e15278cb76a1b056152ea5f205f52401b02dc1158ed3639d60ff24
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:17:19Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.215
    podIPs:
    - ip: 10.244.36.215
    qosClass: Burstable
    startTime: "2023-03-24T16:17:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:22:39Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-g7nlv
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368089661"
    uid: 2413df6f-a2ed-4c14-a316-8145cfb04fe0
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-103
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-sk9rq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-sk9rq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:22:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:23:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:23:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:22:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0f8fb33926703f4bb2219bc5baf8355fd09c48eec8b3df2883e12a45c45d62df
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:22:40Z"
    hostIP: 10.0.0.14
    phase: Running
    podIP: 10.244.37.212
    podIPs:
    - ip: 10.244.37.212
    qosClass: Burstable
    startTime: "2023-03-24T16:22:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-28T05:36:14Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      debug/node-name: q5000-104
      pod-template-generation: "5"
    name: dcgm-exporter-gnx4q
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "376414063"
    uid: eb5cf49d-4cac-4924-b352-464cf63a7f84
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-104
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-l56gs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-l56gs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:36:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:38:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:38:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:36:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2c5735fe740fe3bc26d774f8db4aaa04465b16952d35578e7fa5a34c495ffc18
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-28T05:37:18Z"
    hostIP: 10.0.0.11
    phase: Running
    podIP: 10.244.12.13
    podIPs:
    - ip: 10.244.12.13
    qosClass: Burstable
    startTime: "2023-03-28T05:36:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:23:32Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-gzk4r
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368090862"
    uid: a5aa1f20-7acc-4a18-b7c0-b72a1aaee758
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - v100-301
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kwmqk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: v100-301
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-kwmqk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:23:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:25:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:25:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:23:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://33ed54587781c63a346bde4281daf886401494009d63c7efa735732cf2a6d823
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState:
        terminated:
          containerID: containerd://8765f309aa4ef6cde36577a918b76abe3c615d1fefee7d43cb65bd8474c5ca24
          exitCode: 0
          finishedAt: "2023-03-24T16:24:30Z"
          reason: Completed
          startedAt: "2023-03-24T16:23:34Z"
      name: exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:24:31Z"
    hostIP: 10.0.0.31
    phase: Running
    podIP: 10.244.28.47
    podIPs:
    - ip: 10.244.28.47
    qosClass: Burstable
    startTime: "2023-03-24T16:23:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:16:25Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-h4szg
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368085293"
    uid: c05f4acb-f732-4f65-823f-93a907127155
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-101
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-wthdg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-wthdg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:16:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:17:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:17:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:16:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8e2886d72c05f547ef2e11d3a8c98d3c354b3145c0ded0b3f2dd39b18635851b
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:16:26Z"
    hostIP: 10.0.0.12
    phase: Running
    podIP: 10.244.23.2
    podIPs:
    - ip: 10.244.23.2
    qosClass: Burstable
    startTime: "2023-03-24T16:16:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:21:46Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-n96ls
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "427053802"
    uid: ff007e8e-085b-4f19-a311-0886b87bb4ef
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-103
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-88fgh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-88fgh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:21:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-30T06:59:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-30T06:59:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:21:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0e8ab8e8bce687549a1a83659aa86e0088a9238b70ba6436f6d1ea33dc5764b8
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState:
        terminated:
          containerID: containerd://07a7c00545a408e2a74156adc51ddef0429378e8e6cf63c6f2534f5d220a574d
          exitCode: 0
          finishedAt: "2023-05-30T06:59:08Z"
          reason: Completed
          startedAt: "2023-05-30T06:36:48Z"
      name: exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2023-05-30T06:59:08Z"
    hostIP: 10.0.0.10
    phase: Running
    podIP: 10.244.20.40
    podIPs:
    - ip: 10.244.20.40
    qosClass: Burstable
    startTime: "2023-03-24T16:21:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:12:54Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-rht6c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368082775"
    uid: 8bc2d6a2-7ddb-42b6-ba15-cbe5a6c83e60
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a40-101
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-p8tq6
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-p8tq6
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:12:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:13:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:13:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:12:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://56d6a27277b13ae5fa64999bee0ac3c3846dd9d360eaf31ed1e6ac66eace8fae
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:12:55Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.198
    podIPs:
    - ip: 10.244.13.198
    qosClass: Burstable
    startTime: "2023-03-24T16:12:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:13:46Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-vgqld
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368083337"
    uid: d268e8e8-0d6e-418b-a95e-36ff0bd31b63
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-104
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-c8dtx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-c8dtx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:13:46Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:14:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:14:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:13:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://54ed959e859730a39b6188bf4bcb9f21279f573df8847f89bc03d2fd76956b81
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:13:48Z"
    hostIP: 10.0.0.15
    phase: Running
    podIP: 10.244.26.93
    podIPs:
    - ip: 10.244.26.93
    qosClass: Burstable
    startTime: "2023-03-24T16:13:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:06:36Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-vqplq
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "368081634"
    uid: 1bd0c9b0-c613-4282-ba5e-694c98348348
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-101
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-htcdx
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-htcdx
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:11:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:11:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:11:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:11:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bf50676ef34a746e1ebb77a2ea9188ac18d4d9cc5873831411cf5fe75a1249a4
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:11:10Z"
    hostIP: 10.0.0.8
    phase: Running
    podIP: 10.244.18.199
    podIPs:
    - ip: 10.244.18.199
    qosClass: Burstable
    startTime: "2023-03-24T16:11:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:20:53Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-xs54h
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "391595676"
    uid: a891d299-18b7-4baa-82dd-8fed63320495
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-101
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-q928d
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-q928d
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:20:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:45:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:45:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:20:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://84fd65456dbf8aef24cc8136ba0a536ca6d11bfdf5a0604f30d0054affda2046
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState:
        terminated:
          containerID: containerd://0d92d659fbe293c0142e9f6663d0a44f66db3ab511a35f072b7103e42e72a302
          exitCode: 255
          finishedAt: "2023-04-20T05:43:47Z"
          reason: Unknown
          startedAt: "2023-03-24T16:20:56Z"
      name: exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-20T05:44:40Z"
    hostIP: 10.0.0.18
    phase: Running
    podIP: 10.244.7.112
    podIPs:
    - ip: 10.244.7.112
    qosClass: Burstable
    startTime: "2023-03-24T16:20:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:06:34+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container exporter; nvidia.com/gpu limit for container exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:19:58Z"
    generateName: dcgm-exporter-
    labels:
      app.kubernetes.io/component: dcgm-exporter
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/name: dcgm-exporter
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 5fd5944fd7
      pod-template-generation: "5"
    name: dcgm-exporter-xx6qn
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: dcgm-exporter
      uid: ccf3fb1c-c951-4f6b-839c-d40a22907a1e
    resourceVersion: "385493769"
    uid: 38f56eae-d6f2-4d46-9ab3-f0953fb78433
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-102
    containers:
    - args:
      - -f
      - /etc/dcgm-exporter/exporter-metrics-config-map/metrics
      env:
      - name: DCGM_EXPORTER_KUBERNETES
        value: "true"
      - name: DCGM_EXPORTER_LISTEN
        value: :9400
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      name: exporter
      ports:
      - containerPort: 9400
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 9400
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 200m
          memory: 2Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 1Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          add:
          - SYS_ADMIN
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kubelet/pod-resources
        name: pod-gpu-resources
        readOnly: true
      - mountPath: /etc/dcgm-exporter/exporter-metrics-config-map
        name: exporter-metrics-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qdwtm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: dcgm-exporter
    serviceAccountName: dcgm-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/pod-resources
        type: ""
      name: pod-gpu-resources
    - configMap:
        defaultMode: 420
        name: exporter-metrics-config-map
      name: exporter-metrics-volume
    - name: kube-api-access-qdwtm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:19:59Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:20:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:20:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:19:59Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1a8f006c70f282f7cedc2fa8a312b0116de495cb94bfc3d6d2208b7f62005c37
      image: nvcr.io/nvidia/k8s/dcgm-exporter:3.0.4-3.0.0-ubuntu20.04
      imageID: nvcr.io/nvidia/k8s/dcgm-exporter@sha256:952ec6be586dcff7c4b2936a20b7704c55b91be2b0ddb6d121ce72f5a833e804
      lastState: {}
      name: exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:20:00Z"
    hostIP: 10.0.0.13
    phase: Running
    podIP: 10.244.24.21
    podIPs:
    - ip: 10.244.24.21
    qosClass: Burstable
    startTime: "2023-03-24T16:19:59Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container sia-pushgateway-pushgateway; cpu, memory, nvidia.com/gpu
        limit for container sia-pushgateway-pushgateway'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T15:39:36Z"
    generateName: dev-sia-pushgateway-7694bb4c69-
    labels:
      app: sia-pushgateway
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      chart: sia-pushgateway-0.1.0
      component: pushgateway
      heritage: Helm
      pod-template-hash: 7694bb4c69
      release: dev-sia-pushgateway
    name: dev-sia-pushgateway-7694bb4c69-wwzrt
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: dev-sia-pushgateway-7694bb4c69
      uid: d3d7f910-7a2d-46da-bf01-83378eaec29e
    resourceVersion: "368054880"
    uid: 51b784d5-f280-4aff-b2a0-3a8e0d3536cd
  spec:
    containers:
    - image: prom/pushgateway:v1.4.2
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/healthy
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      name: sia-pushgateway-pushgateway
      ports:
      - containerPort: 9091
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: 9091
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 10
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /data
        name: storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-7lthh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: dev-sia-pushgateway
    serviceAccountName: dev-sia-pushgateway
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage-volume
      persistentVolumeClaim:
        claimName: dev-sia-pushgateway
    - name: kube-api-access-7lthh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cb9e401c6c54b469b9c3655d34571519ecd4cfcaf80937bbdd6f1b40bec00602
      image: docker.io/prom/pushgateway:v1.4.2
      imageID: docker.io/prom/pushgateway@sha256:a684e7c830a4b19e564a93bfc3bf713e85b04ab9dfcab5633c14cbba241f9231
      lastState: {}
      name: sia-pushgateway-pushgateway
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:39:48Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.190
    podIPs:
    - ip: 10.244.36.190
    qosClass: Burstable
    startTime: "2023-03-24T15:39:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container grafana; cpu, memory, nvidia.com/gpu limit for container
        grafana'
    creationTimestamp: "2023-06-07T09:50:58Z"
    generateName: grafana-55c9dcf854-
    labels:
      app: grafana
      app.si-analytics.ai/created-by: kbae
      app.si-analytics.ai/created-by-serviceaccount: "false"
      pod-template-hash: 55c9dcf854
    name: grafana-55c9dcf854-g6xt4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: grafana-55c9dcf854
      uid: 076ddd1f-84ee-449f-a243-dafe6e515052
    resourceVersion: "434494022"
    uid: 8acc0db5-3138-4cf5-bd0b-76e3f1810a47
  spec:
    containers:
    - env:
      - name: GF_SERVER_HTTP_PORT
        value: "3000"
      - name: GF_AUTH_BASIC_ENABLED
        value: "false"
      - name: GF_AUTH_ANONYMOUS_ENABLED
        value: "true"
      - name: GF_AUTH_ANONYMOUS_ORG_ROLE
        value: Admin
      - name: GF_SERVER_ROOT_URL
        value: /
      image: grafana/grafana:latest
      imagePullPolicy: Always
      name: grafana
      ports:
      - containerPort: 3000
        name: grafana
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d2tll
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-d2tll
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-06-07T09:50:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-07T09:51:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-07T09:51:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-06-07T09:50:58Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c33d8d3d6ba1c92b86ba56afd7fb8b7ef37107ed02b0d5942b882d379756b5be
      image: docker.io/grafana/grafana:latest
      imageID: docker.io/grafana/grafana@sha256:35e8e1b76912e4c3bbaa8de01e730aad310acdb51ea0903f84bd098b12b8d861
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-06-07T09:51:06Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.212
    podIPs:
    - ip: 10.244.13.212
    qosClass: Burstable
    startTime: "2023-06-07T09:50:58Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container kube-state-metrics; nvidia.com/gpu limit for container kube-state-metrics'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:41:17Z"
    generateName: ksm-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.6.0
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      app.si-analytics.ai/managed-by: cloud-engineer
      controller-revision-hash: ksm-568d5865c
      helm.sh/chart: kube-state-metrics-4.20.2
      monitoring: "true"
      statefulset.kubernetes.io/pod-name: ksm-0
    name: ksm-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: ksm
      uid: fa1fda9d-82be-4ce4-9888-06b4769795ef
    resourceVersion: "368057322"
    uid: ba8a2f3f-4605-4a96-9e55-6d2b046616bf
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      - --metric-labels-allowlist=configmap=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],deployment=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],daemonset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],cronjobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],jobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],pods=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],service=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],secret=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],statefulset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount]
      - --pod=$(POD_NAME)
      - --pod-namespace=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8081
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-d4pcn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: ksm-0
    nodeName: v100-301
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsUser: 65534
    serviceAccount: ksm
    serviceAccountName: ksm
    subdomain: ksm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-d4pcn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:41:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:42:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:42:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:41:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3578fdf6fb87d4ec280d5143c6252bcd74ef2e988ee48165a2c25271074d9667
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:bdab4e49d71d272cf944c8612dff5ab1250f0fafdae45c22980286ac0c016032
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:43Z"
    hostIP: 10.0.0.31
    phase: Running
    podIP: 10.244.28.36
    podIPs:
    - ip: 10.244.28.36
    qosClass: Burstable
    startTime: "2023-03-24T15:41:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container kube-state-metrics; nvidia.com/gpu limit for container kube-state-metrics'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T15:39:42Z"
    generateName: ksm-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.6.0
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      app.si-analytics.ai/managed-by: cloud-engineer
      controller-revision-hash: ksm-568d5865c
      helm.sh/chart: kube-state-metrics-4.20.2
      monitoring: "true"
      statefulset.kubernetes.io/pod-name: ksm-1
    name: ksm-1
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: ksm
      uid: fa1fda9d-82be-4ce4-9888-06b4769795ef
    resourceVersion: "368055697"
    uid: 1ffe62f8-3403-49e2-9b10-94836c85f246
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      - --metric-labels-allowlist=configmap=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],deployment=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],daemonset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],cronjobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],jobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],pods=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],service=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],secret=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],statefulset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount]
      - --pod=$(POD_NAME)
      - --pod-namespace=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8081
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-65r69
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: ksm-1
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsUser: 65534
    serviceAccount: ksm
    serviceAccountName: ksm
    subdomain: ksm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-65r69
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d721e26b11aeea0f33aa6d56fc7a57e57f66dd4ebeeac96559553140941e02c5
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:bdab4e49d71d272cf944c8612dff5ab1250f0fafdae45c22980286ac0c016032
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:40:44Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.202
    podIPs:
    - ip: 10.244.36.202
    qosClass: Burstable
    startTime: "2023-03-24T15:39:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container kube-state-metrics; nvidia.com/gpu limit for container kube-state-metrics'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T13:21:11Z"
    generateName: ksm-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: kube-state-metrics
      app.kubernetes.io/version: 2.6.0
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      app.si-analytics.ai/managed-by: cloud-engineer
      controller-revision-hash: ksm-568d5865c
      helm.sh/chart: kube-state-metrics-4.20.2
      monitoring: "true"
      statefulset.kubernetes.io/pod-name: ksm-2
    name: ksm-2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: ksm
      uid: fa1fda9d-82be-4ce4-9888-06b4769795ef
    resourceVersion: "367945913"
    uid: 3818a9bc-4cf6-4be3-b231-c663657aec89
  spec:
    containers:
    - args:
      - --port=8080
      - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
      - --metric-labels-allowlist=configmap=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],deployment=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],daemonset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],cronjobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],jobs=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],pods=[app.kubernetes.io/managed-by,gpu,user,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],service=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],secret=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount],statefulset=[app.kubernetes.io/managed-by,app.si-analytics.ai/created-by,app.si-analytics.ai/created-by-serviceaccount,app.si-analytics.ai/last-edited-by,app.si-analytics.ai/last-edited-by-serviceaccount]
      - --pod=$(POD_NAME)
      - --pod-namespace=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kube-state-metrics
      ports:
      - containerPort: 8080
        name: http
        protocol: TCP
      - containerPort: 8081
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hwqjl
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: ksm-2
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsUser: 65534
    serviceAccount: ksm
    serviceAccountName: ksm
    subdomain: ksm
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-hwqjl
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:22:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:22:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T13:21:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f4dafbe56847bb87cc0e83d4020c3999664c2a7d9974a859e5b8cd88a932d5cd
      image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.6.0
      imageID: registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:bdab4e49d71d272cf944c8612dff5ab1250f0fafdae45c22980286ac0c016032
      lastState: {}
      name: kube-state-metrics
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T13:21:50Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.177
    podIPs:
    - ip: 10.244.13.177
    qosClass: Burstable
    startTime: "2023-03-24T13:21:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2022-10-14T13:29:32+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container metrics-server; cpu, memory, nvidia.com/gpu limit for
        container metrics-server'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:11:53Z"
    generateName: metrics-server-855bd4bf64-
    labels:
      app.kubernetes.io/instance: metrics-server
      app.kubernetes.io/name: metrics-server
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: jhlee
      pod-template-hash: 855bd4bf64
    name: metrics-server-855bd4bf64-7g4k4
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-855bd4bf64
      uid: 4ebc6609-ac01-4aca-accf-23f5ae73a396
    resourceVersion: "368030441"
    uid: 451a8679-05b3-4a0c-8233-88551b22421f
  spec:
    containers:
    - args:
      - --secure-port=4443
      - --cert-dir=/tmp
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --kubelet-use-node-status-port
      - --metric-resolution=15s
      - --kubelet-insecure-tls
      image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /livez
          port: https
          scheme: HTTPS
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: metrics-server
      ports:
      - containerPort: 4443
        name: https
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readyz
          port: https
          scheme: HTTPS
        initialDelaySeconds: 20
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - SYS_ADMIN
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1000
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp
        name: tmp
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-zlwbq
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: tmp
    - name: kube-api-access-zlwbq
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:11:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:12:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:12:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:11:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://077abc7a852d02178235cad5887d9973f6297db39feb0a8ae09ad0d0d4412c8c
      image: k8s.gcr.io/metrics-server/metrics-server:v0.6.1
      imageID: k8s.gcr.io/metrics-server/metrics-server@sha256:5ddc6458eb95f5c70bd13fdab90cbd7d6ad1066e5b528ad1dcb28b76c5fb2f00
      lastState: {}
      name: metrics-server
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:11:55Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.182
    podIPs:
    - ip: 10.244.36.182
    qosClass: Burstable
    startTime: "2023-03-24T15:11:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:00Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-7k4jf
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "415624470"
    uid: 5f4d4a12-39be-488b-b125-d9ad91d84755
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-201
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: a100-201
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:13:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:13:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://2de602d9d54aee1fd6c0d40d2b3e5e1df4169160a69607f594f2b7524b17f74e
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://d265881f2e236795f2281795c59f88b8673be4490ffea1203d8a51f24acfb981
          exitCode: 255
          finishedAt: "2023-05-17T10:13:03Z"
          reason: Unknown
          startedAt: "2023-05-17T04:19:44Z"
      name: node-exporter
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2023-05-17T10:13:06Z"
    hostIP: 10.0.0.19
    phase: Running
    podIP: 10.0.0.19
    podIPs:
    - ip: 10.0.0.19
    qosClass: Burstable
    startTime: "2023-03-24T16:05:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-28T05:22:31Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-8x9lz
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "439861152"
    uid: fd765c22-9ca9-4eb5-bbe1-a953dfc37ade
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-104
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: q5000-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:22:31Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T05:14:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T05:14:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-28T05:22:31Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://84685a3fcf625bc4692785af901fa5f6f7917aa9166070c37693b2009878d33d
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://93810acb774ad9ee63247826bf58a5e85cfb8c2879f683e066c50c4d773e8114
          exitCode: 143
          finishedAt: "2023-06-13T05:14:22Z"
          reason: Error
          startedAt: "2023-06-10T03:54:42Z"
      name: node-exporter
      ready: true
      restartCount: 137
      started: true
      state:
        running:
          startedAt: "2023-06-13T05:14:22Z"
    hostIP: 10.0.0.11
    phase: Running
    podIP: 10.0.0.11
    podIPs:
    - ip: 10.0.0.11
    qosClass: Burstable
    startTime: "2023-03-28T05:22:31Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:04:29Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-99cp6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076540"
    uid: e9906739-a4cb-43a1-9be8-cd3b110eda4f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-01
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: dev-master-01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ff85268f7030262f4a468a396929422c21ed562255fb43ecee532e9f384380d0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:29Z"
    hostIP: 10.0.3.101
    phase: Running
    podIP: 10.0.3.101
    podIPs:
    - ip: 10.0.3.101
    qosClass: Burstable
    startTime: "2023-03-24T16:04:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:04:48Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-9rwvr
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "385410891"
    uid: 752d07ad-051c-4a0b-b2f7-aed76d26594d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-202
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g2080-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:49:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:49:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bbdf627397527b1ebbbe5ff3786ef509920fab4a21934142b3dbbc09c7d24ffb
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://dd8d3ccb4ce3fe52ce44640224a20ff1ea7e81759be6b3777698753ef6cfdbb8
          exitCode: 255
          finishedAt: "2023-04-13T04:49:21Z"
          reason: Unknown
          startedAt: "2023-03-24T16:04:49Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:49:33Z"
    hostIP: 10.0.0.26
    phase: Running
    podIP: 10.0.0.26
    podIPs:
    - ip: 10.0.0.26
    qosClass: Burstable
    startTime: "2023-03-24T16:04:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:02Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-c96kh
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077206"
    uid: d783284e-a5c4-4c35-8f67-f36dc9c98f4b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-104
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g3090-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e3f4e6e409cbad5101939c813830f8c947f9b4e297692e5c373c9c1b0e417c9a
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:03Z"
    hostIP: 10.0.0.15
    phase: Running
    podIP: 10.0.0.15
    podIPs:
    - ip: 10.0.0.15
    qosClass: Burstable
    startTime: "2023-03-24T16:05:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:52Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-ck25f
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076972"
    uid: fc304cf1-ea62-43ee-adc9-df6125cfb4e2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - v100-301
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: v100-301
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a31d8c67df777659ccf0292e0a54728b86dc9b2816b147007b707ba82ef2a95b
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:52Z"
    hostIP: 10.0.0.31
    phase: Running
    podIP: 10.0.0.31
    podIPs:
    - ip: 10.0.0.31
    qosClass: Burstable
    startTime: "2023-03-24T16:04:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:57Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-f4dkb
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "421512417"
    uid: e144740e-a7b3-4ed4-8543-94ff19a10e9b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-202
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: a100-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:07:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:07:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://09ea56afbe0cf2ee4ced2b6f49f59e828b80ef072b8b2e8085053f5df652315c
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://dbd87534c9efaf5b10116d9b7ed8ff35196749d63b474701b06dd34f5a6c1db5
          exitCode: 255
          finishedAt: "2023-05-24T05:07:14Z"
          reason: Unknown
          startedAt: "2023-05-24T03:26:19Z"
      name: node-exporter
      ready: true
      restartCount: 13
      started: true
      state:
        running:
          startedAt: "2023-05-24T05:07:21Z"
    hostIP: 10.0.0.20
    phase: Running
    podIP: 10.0.0.20
    podIPs:
    - ip: 10.0.0.20
    qosClass: Burstable
    startTime: "2023-03-24T16:04:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:32Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-hcxkn
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076613"
    uid: 60f06e0e-c44b-47ff-b255-205750b14010
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g3090-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://646f77d044b5a3436be37fe6aadede9a1bccd37a95c653661ddd10f3c31f994e
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:33Z"
    hostIP: 10.0.0.12
    phase: Running
    podIP: 10.0.0.12
    podIPs:
    - ip: 10.0.0.12
    qosClass: Burstable
    startTime: "2023-03-24T16:04:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:12Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-hlkzg
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077374"
    uid: 17d644de-8971-4a78-80cb-b309ac3f89a7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - cpu-102
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: cpu-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:12Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:12Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b3631b91e77d24527ca9e1ddd338a2b4b85e55d34f41989a24d26be472cbfd72
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:13Z"
    hostIP: 10.0.0.22
    phase: Running
    podIP: 10.0.0.22
    podIPs:
    - ip: 10.0.0.22
    qosClass: Burstable
    startTime: "2023-03-24T16:05:12Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:54Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-htkp2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "385404996"
    uid: 78c9d85b-c58b-438b-a494-5087210e59e1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-102
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g2080-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:40:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:40:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4bc7367395d98ab04101380692416f58c30248f047cb5191662225228cd9ac8c
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://7e5478324d1302ff8ec41d4824072a2b08cd4ae44df124c1ea52a1feb5a11837
          exitCode: 255
          finishedAt: "2023-04-13T04:40:18Z"
          reason: Unknown
          startedAt: "2023-03-24T16:04:55Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:40:30Z"
    hostIP: 10.0.0.25
    phase: Running
    podIP: 10.0.0.25
    podIPs:
    - ip: 10.0.0.25
    qosClass: Burstable
    startTime: "2023-03-24T16:04:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:39Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-jktmd
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "427053320"
    uid: 61308ae0-1289-424c-b36c-c7afaf6f5738
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-103
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: q5000-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-30T06:59:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-30T06:59:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://922017e8a6a0599a0021dba0f9fb02ee5dfae9217a1ea626d7227ab93e479e9a
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://b0dd1a52e5ebed30681ff2fe0da702266f6e427532299b4d9a9f8024a3289d75
          exitCode: 143
          finishedAt: "2023-05-30T06:59:08Z"
          reason: Error
          startedAt: "2023-05-30T06:36:48Z"
      name: node-exporter
      ready: true
      restartCount: 2
      started: true
      state:
        running:
          startedAt: "2023-05-30T06:59:08Z"
    hostIP: 10.0.0.10
    phase: Running
    podIP: 10.0.0.10
    podIPs:
    - ip: 10.0.0.10
    qosClass: Burstable
    startTime: "2023-03-24T16:04:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:42Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-kjn7q
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076799"
    uid: 765d9265-c377-46dd-9a51-ced097f5653e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-103
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g3090-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5349d896b389b24e333071e2c9bc8abe2064749c59e965e88e21352756a62858
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:43Z"
    hostIP: 10.0.0.14
    phase: Running
    podIP: 10.0.0.14
    podIPs:
    - ip: 10.0.0.14
    qosClass: Burstable
    startTime: "2023-03-24T16:04:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:36Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-lqsq7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076670"
    uid: 69b10794-eb91-4649-aa5c-d95c07e37e19
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-02
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: dev-master-02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c91be4ac19a963157b7b7c66556865ad007d42458996e35e567fbc3b0b7bbe9b
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:37Z"
    hostIP: 10.0.3.102
    phase: Running
    podIP: 10.0.3.102
    podIPs:
    - ip: 10.0.3.102
    qosClass: Burstable
    startTime: "2023-03-24T16:04:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T16:04:25Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-mpbkc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "391594745"
    uid: a22e77f3-c81d-422c-8904-6d73a48fbd13
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: a100-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:25Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:43:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:43:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c10187b00bea303a6dddbeb9865c53c5393cd73815246194d5afa2f8f5d8e8e5
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState:
        terminated:
          containerID: containerd://50440961440f4ad3ccb43aa05f91c0179c5e92b46e9d4425baac7893201e0864
          exitCode: 255
          finishedAt: "2023-04-20T05:43:47Z"
          reason: Unknown
          startedAt: "2023-03-24T16:04:26Z"
      name: node-exporter
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-20T05:43:51Z"
    hostIP: 10.0.0.18
    phase: Running
    podIP: 10.0.0.18
    podIPs:
    - ip: 10.0.0.18
    qosClass: Burstable
    startTime: "2023-03-24T16:04:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:06Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-mzrsl
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077265"
    uid: e98765ca-fdd0-4f77-bbc7-e1a3dc387f25
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-102
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: q5000-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ff4f368ac4fa532876842fcbe5f252ce54d92a627923b855050d9a4d5a40aaf7
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:07Z"
    hostIP: 10.0.0.9
    phase: Running
    podIP: 10.0.0.9
    podIPs:
    - ip: 10.0.0.9
    qosClass: Burstable
    startTime: "2023-03-24T16:05:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:21Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-nhc77
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076416"
    uid: ebb062cf-e720-4b70-9cb6-8f84e860aad1
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-102
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: g3090-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:22Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:24Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:24Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:22Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ae8553193317782b08f3342fe4e7a55be70a37416d3e95b05565fff6a851f6cb
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:22Z"
    hostIP: 10.0.0.13
    phase: Running
    podIP: 10.0.0.13
    podIPs:
    - ip: 10.0.0.13
    qosClass: Burstable
    startTime: "2023-03-24T16:04:22Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:18Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-qdg8r
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076347"
    uid: 203b9193-75a7-42a0-9221-65bd2e99c7f2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - cpu-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: cpu-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:18Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:18Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://204c9337c9c9a035356c01edff454ea4031b44343f3bb4d17308ff6dfea84185
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:19Z"
    hostIP: 10.0.0.21
    phase: Running
    podIP: 10.0.0.21
    podIPs:
    - ip: 10.0.0.21
    qosClass: Burstable
    startTime: "2023-03-24T16:04:18Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:15Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-sbp2b
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077448"
    uid: edd89f20-186d-4034-bfdd-e871e726bc2c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: q5000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://eda788d6e86da801c25c2ba20cb03981350127454b645f97914d88ffe503b568
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:16Z"
    hostIP: 10.0.0.8
    phase: Running
    podIP: 10.0.0.8
    podIPs:
    - ip: 10.0.0.8
    qosClass: Burstable
    startTime: "2023-03-24T16:05:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:19Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-t9nfp
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077507"
    uid: 7a6f6e6b-9810-4745-92ce-cd55616b6b9a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a6000-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4b8f875355da7cdb80cd2a2c74abe9b07df3d3bb42e25011403fc55bde4a95ce
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:19Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.0.0.23
    podIPs:
    - ip: 10.0.0.23
    qosClass: Burstable
    startTime: "2023-03-24T16:05:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:05:09Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-vchz9
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368077314"
    uid: a2dacd19-dae4-4ec3-8dc9-b7bae04cc57b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-03
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: dev-master-03
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:05:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a1ed3024eeeaef161d12252f2b8ee0c70c9a2234bf9ae5d2bd0955b0c4619d74
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:05:10Z"
    hostIP: 10.0.3.103
    phase: Running
    podIP: 10.0.3.103
    podIPs:
    - ip: 10.0.3.103
    qosClass: Burstable
    startTime: "2023-03-24T16:05:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:04:10+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container node-exporter; nvidia.com/gpu limit for container node-exporter'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T16:04:16Z"
    generateName: node-exporter-
    labels:
      app.kubernetes.io/component: metrics
      app.kubernetes.io/instance: dev-sia-exporters
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: node-exporter
      app.kubernetes.io/version: 1.3.1
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 77c75d5b96
      helm.sh/chart: node-exporter-4.4.1
      pod-template-generation: "11"
    name: node-exporter-x4svj
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: node-exporter
      uid: 34e53622-2814-48b1-871b-cf1c2c42c25c
    resourceVersion: "368076291"
    uid: a01db120-0f0a-4e6f-bef6-717108524d8e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a40-101
    automountServiceAccountToken: false
    containers:
    - args:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
      - --path.rootfs=/host/root
      - --web.listen-address=[$(HOST_IP)]:9100
      env:
      - name: HOST_IP
        value: 0.0.0.0
      image: quay.io/prometheus/node-exporter:v1.3.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: node-exporter
      ports:
      - containerPort: 9100
        hostPort: 9100
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9100
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 880m
          memory: 1Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 440m
          memory: 512Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /host/proc
        name: proc
        readOnly: true
      - mountPath: /host/sys
        name: sys
        readOnly: true
      - mountPath: /host/root
        mountPropagation: HostToContainer
        name: root
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    hostPID: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: node-exporter
    serviceAccountName: node-exporter
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /proc
        type: ""
      name: proc
    - hostPath:
        path: /sys
        type: ""
      name: sys
    - hostPath:
        path: /
        type: ""
      name: root
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T16:04:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b8848c8cf1c4afb39f83a24a0f5ac79a5b528911d9462b4fa7e54449f785ea0d
      image: quay.io/prometheus/node-exporter:v1.3.1
      imageID: quay.io/prometheus/node-exporter@sha256:f2269e73124dd0f60a7d19a2ce1264d33d08a985aed0ee6b0b89d0be470592cd
      lastState: {}
      name: node-exporter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T16:04:17Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.0.0.30
    podIPs:
    - ip: 10.0.0.30
    qosClass: Burstable
    startTime: "2023-03-24T16:04:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/restartedAt: "2023-06-09T14:03:39+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container prometheus; cpu, memory, nvidia.com/gpu limit for container
        prometheus'
    creationTimestamp: "2023-06-09T05:03:36Z"
    generateName: prometheus-deployment-6d74ff8fd-
    labels:
      app: prometheus-server
      app.si-analytics.ai/created-by: kbae
      app.si-analytics.ai/created-by-serviceaccount: "false"
      app.si-analytics.ai/last-edited-by: kbae
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      pod-template-hash: 6d74ff8fd
    name: prometheus-deployment-6d74ff8fd-qlrm5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: prometheus-deployment-6d74ff8fd
      uid: eb35bcea-a159-4e53-8ecb-6335ce0b8564
    resourceVersion: "436165071"
    uid: 18106abc-a52d-47e5-aea9-f21e9eb2e889
  spec:
    containers:
    - args:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus/
      - --storage.tsdb.retention.time=1d
      image: prom/prometheus:latest
      imagePullPolicy: Always
      name: prometheus
      ports:
      - containerPort: 9090
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/prometheus/
        name: prometheus-config-volume
      - mountPath: /prometheus/
        name: prometheus-storage-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-fjc26
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: prometheus-server-conf
      name: prometheus-config-volume
    - emptyDir: {}
      name: prometheus-storage-volume
    - name: kube-api-access-fjc26
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-06-09T05:03:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-09T05:03:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-09T05:03:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-06-09T05:03:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e83218c366b4bf71c41123d7132dd59c3bbfa03c20b9d36eaaeb08e69aff7e54
      image: docker.io/prom/prometheus:latest
      imageID: docker.io/prom/prometheus@sha256:0f0b7feb6f02620df7d493ad7437b6ee95b6d16d8d18799f3607124e501444b1
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-06-09T05:03:38Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.241
    podIPs:
    - ip: 10.244.13.241
    qosClass: Burstable
    startTime: "2023-06-09T05:03:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container prometheus; nvidia.com/gpu limit for container prometheus; nvidia.com/gpu
        request for container config-reloader; nvidia.com/gpu limit for container
        config-reloader; cpu, memory, nvidia.com/gpu request for container thanos-sidecar;
        cpu, memory, nvidia.com/gpu limit for container thanos-sidecar; nvidia.com/gpu
        request for init container init-config-reloader; nvidia.com/gpu limit for
        init container init-config-reloader'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-01T08:18:46Z"
    generateName: prometheus-kps-prometheus-
    labels:
      app.kubernetes.io/instance: kps-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.32.1
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      controller-revision-hash: prometheus-kps-prometheus-6b6667698b
      operator.prometheus.io/name: kps-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kps-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kps-prometheus-0
    name: prometheus-kps-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-kps-prometheus
      uid: 734b12c8-717f-4675-84d3-63c930d426cd
    resourceVersion: "382594619"
    uid: 87b663f7-ae2a-42af-a70f-a831573ddeef
  spec:
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=10d
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus.dev.sia-service.kr/
      - --web.route-prefix=/
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      - --storage.tsdb.max-block-duration=2h
      - --storage.tsdb.min-block-duration=2h
      image: quay.io/prometheus/prometheus:v2.32.1
      imagePullPolicy: IfNotPresent
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "4"
          memory: 15Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: "4"
          memory: 15Gi
          nvidia.com/gpu: "0"
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-kps-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hh6nj
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hh6nj
        readOnly: true
    - args:
      - sidecar
      - --prometheus.url=http://127.0.0.1:9090/
      - --grpc-address=:10901
      - --http-address=:10902
      - --objstore.config=$(OBJSTORE_CONFIG)
      - --tsdb.path=/prometheus
      - --log.level=info
      - --log.format=logfmt
      env:
      - name: OBJSTORE_CONFIG
        valueFrom:
          secretKeyRef:
            key: objstore.yml
            name: thanos-objstore
      image: quay.io/thanos/thanos:v0.23.1
      imagePullPolicy: IfNotPresent
      name: thanos-sidecar
      ports:
      - containerPort: 10902
        name: http
        protocol: TCP
      - containerPort: 10901
        name: grpc
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /prometheus
        name: prometheus-kps-prometheus-db
        subPath: prometheus-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hh6nj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-kps-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hh6nj
        readOnly: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: kps-prometheus
    serviceAccountName: kps-prometheus
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-kps-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-kps-prometheus-db-prometheus-kps-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-kps-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-kps-prometheus-tls-assets-0
    - emptyDir: {}
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-kps-prometheus-rulefiles-0
      name: prometheus-kps-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-kps-prometheus-web-config
    - name: kube-api-access-hh6nj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T08:18:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-10T00:33:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-10T00:33:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-01T08:18:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3c2b1acb391d66d398efa4c5bf3a34834d4f84bbce8697eaac5f10b0b8e6f480
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState:
        terminated:
          containerID: containerd://90d13ccb5cde4a6f695ce93d2a26fafdb93b8cdbc0dd8040ecbf51c17cb1cdf5
          exitCode: 2
          finishedAt: "2023-04-08T14:01:50Z"
          message: |
            level=info ts=2023-03-24T13:19:20.263523689Z caller=main.go:109 msg="Starting prometheus-config-reloader" version="(version=0.53.1, branch=refs/tags/v0.53.1, revision=d8ba1c766a141cb35072ae2f2578ec8588c9efcd)"
            level=info ts=2023-03-24T13:19:20.263627785Z caller=main.go:110 build_context="(go=go1.17.5, user=Action-Run-ID-1602303931, date=20211220-13:48:09)"
            level=info ts=2023-03-24T13:19:20.263871024Z caller=main.go:144 msg="Starting web server for metrics" listen=:8080
            level=error ts=2023-03-24T13:19:20.359057751Z caller=runutil.go:101 msg="function failed. Retrying in next tick" err="trigger reload: reload request failed: Post \"http://127.0.0.1:9090/-/reload\": dial tcp 127.0.0.1:9090: connect: connection refused"
            level=info ts=2023-03-24T13:22:40.971020476Z caller=reloader.go:373 msg="Reload triggered" cfg_in=/etc/prometheus/config/prometheus.yaml.gz cfg_out=/etc/prometheus/config_out/prometheus.env.yaml watched_dirs=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
            level=info ts=2023-03-24T13:22:40.971133359Z caller=reloader.go:235 msg="started watching config file and directories for changes" cfg=/etc/prometheus/config/prometheus.yaml.gz out=/etc/prometheus/config_out/prometheus.env.yaml dirs=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
          reason: Error
          startedAt: "2023-03-24T13:19:20Z"
      name: config-reloader
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2023-04-08T14:01:51Z"
    - containerID: containerd://eea53bb6d4a9266d5970f78eb1e3f9792da95ab29b20007d9867aa3b1222663c
      image: quay.io/prometheus/prometheus:v2.32.1
      imageID: quay.io/prometheus/prometheus@sha256:cb9817249c346d6cfadebe383ed3b3cd4c540f623db40c4ca00da2ada45259bb
      lastState:
        terminated:
          containerID: containerd://ca6177a56425126241e68d27cc12432cfc1d439a6562e3e393df16f847583db8
          exitCode: 1
          finishedAt: "2023-04-10T00:32:10Z"
          message: |
            or, attempting repair" err="read records: corruption in segment /prometheus/wal/00013703 at 62774456: unexpected full record"
            ts=2023-04-10T00:32:10.196Z caller=wal.go:364 level=warn component=tsdb msg="Starting corruption repair" segment=13703 offset=62774456
            ts=2023-04-10T00:32:10.196Z caller=wal.go:372 level=warn component=tsdb msg="Deleting all segments newer than corrupted segment" segment=13703
            ts=2023-04-10T00:32:10.214Z caller=wal.go:394 level=warn component=tsdb msg="Rewrite corrupted segment" segment=13703
            ts=2023-04-10T00:32:10.325Z caller=wal.go:777 level=error component=tsdb msg="sync previous segment" err="sync /prometheus/wal/00013705: file already closed"
            ts=2023-04-10T00:32:10.325Z caller=wal.go:780 level=error component=tsdb msg="close previous segment" err="close /prometheus/wal/00013705: file already closed"
            ts=2023-04-10T00:32:10.325Z caller=main.go:799 level=info msg="Stopping scrape discovery manager..."
            ts=2023-04-10T00:32:10.325Z caller=main.go:813 level=info msg="Stopping notify discovery manager..."
            ts=2023-04-10T00:32:10.325Z caller=main.go:835 level=info msg="Stopping scrape manager..."
            ts=2023-04-10T00:32:10.325Z caller=main.go:809 level=info msg="Notify discovery manager stopped"
            ts=2023-04-10T00:32:10.325Z caller=main.go:795 level=info msg="Scrape discovery manager stopped"
            ts=2023-04-10T00:32:10.325Z caller=manager.go:945 level=info component="rule manager" msg="Stopping rule manager..."
            ts=2023-04-10T00:32:10.326Z caller=main.go:829 level=info msg="Scrape manager stopped"
            ts=2023-04-10T00:32:10.326Z caller=manager.go:955 level=info component="rule manager" msg="Rule manager stopped"
            ts=2023-04-10T00:32:10.326Z caller=notifier.go:600 level=info component=notifier msg="Stopping notification manager..."
            ts=2023-04-10T00:32:10.326Z caller=main.go:1055 level=info msg="Notifier manager stopped"
            ts=2023-04-10T00:32:10.326Z caller=main.go:1064 level=error err="opening storage failed: repair corrupted WAL: rename /prometheus/wal/00013703 /prometheus/wal/00013703.repair: remote I/O error"
          reason: Error
          startedAt: "2023-04-10T00:31:48Z"
      name: prometheus
      ready: true
      restartCount: 7
      started: true
      state:
        running:
          startedAt: "2023-04-10T00:32:56Z"
    - containerID: containerd://27aa67c3fbb95389247ca382eeab0377de4b49e1461dce370e317ae515c330b4
      image: quay.io/thanos/thanos:v0.23.1
      imageID: quay.io/thanos/thanos@sha256:2f7d1ddc7877b076efbc3fa626b5003f7f197efbd777cff0eec2b20c2cd68d20
      lastState:
        terminated:
          containerID: containerd://9872aaf303d2a242b7da9f1fb788acfa7648f6f677d7842e6cea001d2b5a3fc5
          exitCode: 0
          finishedAt: "2023-04-08T14:01:50Z"
          reason: Completed
          startedAt: "2023-03-24T13:19:26Z"
      name: thanos-sidecar
      ready: true
      restartCount: 3
      started: true
      state:
        running:
          startedAt: "2023-04-08T14:01:52Z"
    hostIP: 10.0.0.30
    initContainerStatuses:
    - containerID: containerd://8655448a23bf7b3c578392ae9b51f127bde0877f579afb927d03861ab15696c2
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 3
      state:
        terminated:
          containerID: containerd://8655448a23bf7b3c578392ae9b51f127bde0877f579afb927d03861ab15696c2
          exitCode: 0
          finishedAt: "2023-04-08T14:01:51Z"
          reason: Completed
          startedAt: "2023-04-08T14:01:51Z"
    phase: Running
    podIP: 10.244.13.15
    podIPs:
    - ip: 10.244.13.15
    qosClass: Burstable
    startTime: "2023-03-01T08:18:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container prometheus; nvidia.com/gpu limit for container prometheus; nvidia.com/gpu
        request for container config-reloader; nvidia.com/gpu limit for container
        config-reloader; cpu, memory, nvidia.com/gpu request for container thanos-sidecar;
        cpu, memory, nvidia.com/gpu limit for container thanos-sidecar; nvidia.com/gpu
        request for init container init-config-reloader; nvidia.com/gpu limit for
        init container init-config-reloader'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:39:50Z"
    generateName: prometheus-kps-prometheus-
    labels:
      app.kubernetes.io/instance: kps-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.32.1
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      controller-revision-hash: prometheus-kps-prometheus-6b6667698b
      operator.prometheus.io/name: kps-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: kps-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-kps-prometheus-1
    name: prometheus-kps-prometheus-1
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-kps-prometheus
      uid: 734b12c8-717f-4675-84d3-63c930d426cd
    resourceVersion: "370622564"
    uid: 149fc407-947e-4217-84e0-986ba4e82d2f
  spec:
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=10d
      - --web.enable-lifecycle
      - --web.external-url=http://prometheus.dev.sia-service.kr/
      - --web.route-prefix=/
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      - --storage.tsdb.max-block-duration=2h
      - --storage.tsdb.min-block-duration=2h
      image: quay.io/prometheus/prometheus:v2.32.1
      imagePullPolicy: IfNotPresent
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "4"
          memory: 15Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: "4"
          memory: 15Gi
          nvidia.com/gpu: "0"
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-kps-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9zql
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9zql
        readOnly: true
    - args:
      - sidecar
      - --prometheus.url=http://127.0.0.1:9090/
      - --grpc-address=:10901
      - --http-address=:10902
      - --objstore.config=$(OBJSTORE_CONFIG)
      - --tsdb.path=/prometheus
      - --log.level=info
      - --log.format=logfmt
      env:
      - name: OBJSTORE_CONFIG
        valueFrom:
          secretKeyRef:
            key: objstore.yml
            name: thanos-objstore
      image: quay.io/thanos/thanos:v0.23.1
      imagePullPolicy: IfNotPresent
      name: thanos-sidecar
      ports:
      - containerPort: 10902
        name: http
        protocol: TCP
      - containerPort: 10901
        name: grpc
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /prometheus
        name: prometheus-kps-prometheus-db
        subPath: prometheus-db
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9zql
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-kps-prometheus-1
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-kps-prometheus-rulefiles-0
        name: prometheus-kps-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-x9zql
        readOnly: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: kps-prometheus
    serviceAccountName: kps-prometheus
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-kps-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-kps-prometheus-db-prometheus-kps-prometheus-1
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-kps-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-kps-prometheus-tls-assets-0
    - emptyDir: {}
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-kps-prometheus-rulefiles-0
      name: prometheus-kps-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-kps-prometheus-web-config
    - name: kube-api-access-x9zql
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:58Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-27T13:32:11Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-27T13:32:11Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://69319592a4020b52f6cd498cb1720ddfed4a7a551ceb5b13e5968357d3cf29b0
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:10Z"
    - containerID: containerd://cff676f5eaf165dbbcb563d89fe38abdabaa9acf76df3c4a7ecda44d2ee1b6f9
      image: quay.io/prometheus/prometheus:v2.32.1
      imageID: quay.io/prometheus/prometheus@sha256:cb9817249c346d6cfadebe383ed3b3cd4c540f623db40c4ca00da2ada45259bb
      lastState: {}
      name: prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:10Z"
    - containerID: containerd://d01e6082134bdfd65cfc8915bd9f12a95ba9d85eefeb97b41702e89ea2521475
      image: quay.io/thanos/thanos:v0.23.1
      imageID: quay.io/thanos/thanos@sha256:2f7d1ddc7877b076efbc3fa626b5003f7f197efbd777cff0eec2b20c2cd68d20
      lastState: {}
      name: thanos-sidecar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:21Z"
    hostIP: 10.0.0.23
    initContainerStatuses:
    - containerID: containerd://7667b2ac2666c66e135fe888882d43f5e7d2a209cd290180961a506919c1feb7
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://7667b2ac2666c66e135fe888882d43f5e7d2a209cd290180961a506919c1feb7
          exitCode: 0
          finishedAt: "2023-03-24T15:40:58Z"
          reason: Completed
          startedAt: "2023-03-24T15:40:58Z"
    phase: Running
    podIP: 10.244.36.207
    podIPs:
    - ip: 10.244.36.207
    qosClass: Burstable
    startTime: "2023-03-24T15:39:50Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/default-container: prometheus
      kubectl.kubernetes.io/restartedAt: "2022-10-11T10:24:04+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container prometheus; cpu, nvidia.com/gpu limit for container prometheus;
        nvidia.com/gpu request for container config-reloader; nvidia.com/gpu limit
        for container config-reloader; nvidia.com/gpu request for init container init-config-reloader;
        nvidia.com/gpu limit for init container init-config-reloader'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-24T15:39:45Z"
    generateName: prometheus-sia-dev-kube-prometheus-st-prometheus-
    labels:
      app.kubernetes.io/instance: sia-dev-kube-prometheus-st-prometheus
      app.kubernetes.io/managed-by: prometheus-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/version: 2.32.1
      app.si-analytics.ai/created-by: statefulset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      controller-revision-hash: prometheus-sia-dev-kube-prometheus-st-prometheus-7dbcc7df4
      operator.prometheus.io/name: sia-dev-kube-prometheus-st-prometheus
      operator.prometheus.io/shard: "0"
      prometheus: sia-dev-kube-prometheus-st-prometheus
      statefulset.kubernetes.io/pod-name: prometheus-sia-dev-kube-prometheus-st-prometheus-0
    name: prometheus-sia-dev-kube-prometheus-st-prometheus-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: prometheus-sia-dev-kube-prometheus-st-prometheus
      uid: 48537a5f-debc-483d-ae66-a82b0cfe7e8a
    resourceVersion: "409984445"
    uid: f71c7502-87c9-4dc9-94d6-13d16bc4cd66
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - prometheus
            - key: prometheus
              operator: In
              values:
              - sia-dev-kube-prometheus-st-prometheus
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --web.console.templates=/etc/prometheus/consoles
      - --web.console.libraries=/etc/prometheus/console_libraries
      - --config.file=/etc/prometheus/config_out/prometheus.env.yaml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=90d
      - --web.enable-lifecycle
      - --web.external-url=http://dev-prometheus.sia-service.kr/
      - --web.route-prefix=/
      - --web.config.file=/etc/prometheus/web_config/web-config.yaml
      image: quay.io/prometheus/prometheus:v2.32.1
      imagePullPolicy: IfNotPresent
      name: prometheus
      ports:
      - containerPort: 9090
        name: http-web
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 3
      resources:
        limits:
          cpu: "1"
          memory: 30Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      startupProbe:
        failureThreshold: 60
        httpGet:
          path: /-/ready
          port: http-web
          scheme: HTTP
        periodSeconds: 15
        successThreshold: 1
        timeoutSeconds: 3
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config_out
        name: config-out
        readOnly: true
      - mountPath: /etc/prometheus/certs
        name: tls-assets
        readOnly: true
      - mountPath: /prometheus
        name: prometheus-sia-dev-kube-prometheus-st-prometheus-db
        subPath: prometheus-db
      - mountPath: /etc/prometheus/rules/prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
        name: prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      - mountPath: /etc/prometheus/web_config/web-config.yaml
        name: web-config
        readOnly: true
        subPath: web-config.yaml
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kw4fg
        readOnly: true
    - args:
      - --listen-address=:8080
      - --reload-url=http://127.0.0.1:9090/-/reload
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
        name: prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kw4fg
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: prometheus-sia-dev-kube-prometheus-st-prometheus-0
    initContainers:
    - args:
      - --watch-interval=0
      - --listen-address=:8080
      - --config-file=/etc/prometheus/config/prometheus.yaml.gz
      - --config-envsubst-file=/etc/prometheus/config_out/prometheus.env.yaml
      - --watched-dir=/etc/prometheus/rules/prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      command:
      - /bin/prometheus-config-reloader
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: SHARD
        value: "0"
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imagePullPolicy: IfNotPresent
      name: init-config-reloader
      ports:
      - containerPort: 8080
        name: reloader-web
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 100m
          memory: 50Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: FallbackToLogsOnError
      volumeMounts:
      - mountPath: /etc/prometheus/config
        name: config
      - mountPath: /etc/prometheus/config_out
        name: config-out
      - mountPath: /etc/prometheus/rules/prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
        name: prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-kw4fg
        readOnly: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    serviceAccount: sia-dev-kube-prometheus-st-prometheus
    serviceAccountName: sia-dev-kube-prometheus-st-prometheus
    subdomain: prometheus-operated
    terminationGracePeriodSeconds: 600
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: prometheus-sia-dev-kube-prometheus-st-prometheus-db
      persistentVolumeClaim:
        claimName: prometheus-sia-dev-kube-prometheus-st-prometheus-db-prometheus-sia-dev-kube-prometheus-st-prometheus-0
    - name: config
      secret:
        defaultMode: 420
        secretName: prometheus-sia-dev-kube-prometheus-st-prometheus
    - name: tls-assets
      projected:
        defaultMode: 420
        sources:
        - secret:
            name: prometheus-sia-dev-kube-prometheus-st-prometheus-tls-assets-0
    - emptyDir: {}
      name: config-out
    - configMap:
        defaultMode: 420
        name: prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
      name: prometheus-sia-dev-kube-prometheus-st-prometheus-rulefiles-0
    - name: web-config
      secret:
        defaultMode: 420
        secretName: prometheus-sia-dev-kube-prometheus-st-prometheus-web-config
    - name: kube-api-access-kw4fg
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-11T06:09:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-11T06:09:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:45Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ea2aea9bab1e265aea27438e75387bbf12aca8bf298c3c5fcc56e411f0c5f340
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: config-reloader
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:09Z"
    - containerID: containerd://b06767997dd124f89a9c4915afce044ab47fc8d3991911a78b0868995cba24b7
      image: quay.io/prometheus/prometheus:v2.32.1
      imageID: quay.io/prometheus/prometheus@sha256:cb9817249c346d6cfadebe383ed3b3cd4c540f623db40c4ca00da2ada45259bb
      lastState:
        terminated:
          containerID: containerd://2c3a570d3533ceb1b9a2bd94479a72be9ea411c5c7d26d1cf351d80ecc836b00
          exitCode: 2
          finishedAt: "2023-04-10T00:31:00Z"
          message: "\"\nts=2023-04-10T00:30:58.147Z caller=scrape.go:1252 level=error
            component=\"scrape manager\" scrape_pool=serviceMonitor/dev/test-glc-scene-tile-server/0
            target=http://10.244.20.25:80/metrics msg=\"Scrape commit failed\" err=\"write
            to WAL: log samples: write /prometheus/wal/00053993: no space left on
            device\"\nts=2023-04-10T00:30:58.932Z caller=scrape.go:1252 level=error
            component=\"scrape manager\" scrape_pool=serviceMonitor/dev/backend-kap-scene-tile-server/0
            target=http://10.244.26.131:80/metrics msg=\"Scrape commit failed\" err=\"write
            to WAL: log samples: write /prometheus/wal/00053993: no space left on
            device\"\npanic: write /prometheus/chunks_head/014931: no space left on
            device\n\ngoroutine 3268 [running]:\ngithub.com/prometheus/prometheus/tsdb.(*memSeries).mmapCurrentHeadChunk(0xc013cb6c40,
            0x61ed8995)\n\t/app/tsdb/head_append.go:577 +0x189\ngithub.com/prometheus/prometheus/tsdb.(*memSeries).cutNewHeadChunk(0xc013cb6c40,
            0x1876892ba45, 0x4035000000000000)\n\t/app/tsdb/head_append.go:548 +0x2a\ngithub.com/prometheus/prometheus/tsdb.(*memSeries).append(0xc013cb6c40,
            0x1876892ba45, 0x4035000000000000, 0x195e56e, 0x2169112)\n\t/app/tsdb/head_append.go:515
            +0x1b2\ngithub.com/prometheus/prometheus/tsdb.(*headAppender).Commit(0xc02e91fcc0)\n\t/app/tsdb/head_append.go:451
            +0x685\ngithub.com/prometheus/prometheus/tsdb.dbAppender.Commit({{0x35a19e8,
            0xc02e91fcc0}, 0xc000579040})\n\t/app/tsdb/db.go:857 +0x35\ngithub.com/prometheus/prometheus/storage.(*fanoutAppender).Commit(0xc06b510900)\n\t/app/storage/fanout.go:176
            +0x3f\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).scrapeAndReport.func1()\n\t/app/scrape/scrape.go:1250
            +0x45\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).scrapeAndReport(0xc04f697e10,
            {0xedbc54fc3, 0x4d8e460, 0x4d8e460}, {0xedbc54fc3, 0x4d8e460, 0x4d8e460},
            0x0)\n\t/app/scrape/scrape.go:1321 +0xf71\ngithub.com/prometheus/prometheus/scrape.(*scrapeLoop).run(0xc04f697e10,
            0xc033e82fb8)\n\t/app/scrape/scrape.go:1203 +0x351\ncreated by github.com/prometheus/prometheus/scrape.(*scrapePool).sync\n\t/app/scrape/scrape.go:584
            +0xa55\n"
          reason: Error
          startedAt: "2023-03-24T15:41:09Z"
      name: prometheus
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-10T00:31:00Z"
    hostIP: 10.0.0.23
    initContainerStatuses:
    - containerID: containerd://3c141b1ca93fed96c80b2940fe8bf7469e742126d666b396e365544e69ff8426
      image: quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-config-reloader@sha256:72f4616b02188261f433019734eb25dc60338299f644b26cc1b2d701dd5d888e
      lastState: {}
      name: init-config-reloader
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://3c141b1ca93fed96c80b2940fe8bf7469e742126d666b396e365544e69ff8426
          exitCode: 0
          finishedAt: "2023-03-24T15:40:54Z"
          reason: Completed
          startedAt: "2023-03-24T15:40:54Z"
    phase: Running
    podIP: 10.244.36.205
    podIPs:
    - ip: 10.244.36.205
    qosClass: Burstable
    startTime: "2023-03-24T15:39:45Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container create; cpu, memory, nvidia.com/gpu limit for container
        create'
    creationTimestamp: "2023-04-13T05:39:16Z"
    generateName: sia-dev-kube-prometheus-st-admission-create-
    labels:
      app: kube-prometheus-stack-admission-create
      app.kubernetes.io/instance: sia-dev-kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 26.1.0
      app.si-analytics.ai/created-by: haeram.kim1
      app.si-analytics.ai/created-by-serviceaccount: "false"
      chart: kube-prometheus-stack-26.1.0
      controller-uid: cd71a4d4-2c07-4dc0-8eff-b4393f00f09b
      heritage: Helm
      job-name: sia-dev-kube-prometheus-st-admission-create
      monitoring: "true"
      release: sia-dev-kube-prometheus-stack
    name: sia-dev-kube-prometheus-st-admission-create-vz2w6
    namespace: monitoring
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: sia-dev-kube-prometheus-st-admission-create
      uid: cd71a4d4-2c07-4dc0-8eff-b4393f00f09b
    resourceVersion: "385442705"
    uid: 1ddac383-dcdd-47b8-b80c-a281121a8387
  spec:
    containers:
    - args:
      - create
      - --host=sia-dev-kube-prometheus-st-operator,sia-dev-kube-prometheus-st-operator.monitoring.svc
      - --namespace=monitoring
      - --secret-name=sia-dev-kube-prometheus-st-admission
      image: k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.0@sha256:f3b6b39a6062328c095337b4cadcefd1612348fdd5190b1dcbcb9b9e90bd8068
      imagePullPolicy: IfNotPresent
      name: create
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-qvv2k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 2000
    serviceAccount: sia-dev-kube-prometheus-st-admission
    serviceAccountName: sia-dev-kube-prometheus-st-admission
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-qvv2k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T05:39:16Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T05:39:20Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T05:39:20Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T05:39:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cfaa01edf2d3ddccdce84dd012e3bdc55743c36ef5a347171b874b4ded396ac4
      image: sha256:17e55ec30f203e6acb1e2d35bf8af5e171b3734539e1d2b560c8e80f6b1b259a
      imageID: k8s.gcr.io/ingress-nginx/kube-webhook-certgen@sha256:f3b6b39a6062328c095337b4cadcefd1612348fdd5190b1dcbcb9b9e90bd8068
      lastState: {}
      name: create
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://cfaa01edf2d3ddccdce84dd012e3bdc55743c36ef5a347171b874b4ded396ac4
          exitCode: 0
          finishedAt: "2023-04-13T05:39:19Z"
          reason: Completed
          startedAt: "2023-04-13T05:39:19Z"
    hostIP: 10.0.0.30
    phase: Succeeded
    podIP: 10.244.13.36
    podIPs:
    - ip: 10.244.13.36
    qosClass: Burstable
    startTime: "2023-04-13T05:39:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container kube-prometheus-stack; cpu, memory, nvidia.com/gpu limit
        for container kube-prometheus-stack'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:11:55Z"
    generateName: sia-dev-kube-prometheus-st-operator-59bcfd4797-
    labels:
      app: kube-prometheus-stack-operator
      app.kubernetes.io/instance: sia-dev-kube-prometheus-stack
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/part-of: kube-prometheus-stack
      app.kubernetes.io/version: 26.1.0
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      chart: kube-prometheus-stack-26.1.0
      heritage: Helm
      monitoring: "true"
      pod-template-hash: 59bcfd4797
      release: sia-dev-kube-prometheus-stack
    name: sia-dev-kube-prometheus-st-operator-59bcfd4797-8jk98
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sia-dev-kube-prometheus-st-operator-59bcfd4797
      uid: 0079bdea-e2b7-4b8c-b8bc-7408b3cfc597
    resourceVersion: "368029958"
    uid: cbe613b5-4da0-4172-8715-6aeecb077bb6
  spec:
    containers:
    - args:
      - --kubelet-service=kube-system/sia-dev-kube-prometheus-st-kubelet
      - --localhost=127.0.0.1
      - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.53.1
      - --config-reloader-cpu-request=100m
      - --config-reloader-cpu-limit=100m
      - --config-reloader-memory-request=50Mi
      - --config-reloader-memory-limit=50Mi
      - --thanos-default-base-image=quay.io/thanos/thanos:v0.23.1
      - --web.enable-tls=true
      - --web.cert-file=/cert/cert
      - --web.key-file=/cert/key
      - --web.listen-address=:10250
      - --web.tls-min-version=VersionTLS13
      image: quay.io/prometheus-operator/prometheus-operator:v0.53.1
      imagePullPolicy: IfNotPresent
      name: kube-prometheus-stack
      ports:
      - containerPort: 10250
        name: https
        protocol: TCP
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - SYS_ADMIN
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /cert
        name: tls-secret
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rsqbr
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      runAsGroup: 65534
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: sia-dev-kube-prometheus-st-operator
    serviceAccountName: sia-dev-kube-prometheus-st-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: tls-secret
      secret:
        defaultMode: 420
        secretName: sia-dev-kube-prometheus-st-admission
    - name: kube-api-access-rsqbr
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:11:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:12:04Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:12:04Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:11:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://446145cc1d7b94a5a120a195d8b840d96cd782ea29a2e7efe4d76384fe06a4fb
      image: quay.io/prometheus-operator/prometheus-operator:v0.53.1
      imageID: quay.io/prometheus-operator/prometheus-operator@sha256:e652e4156a1db67e9e2c39cfc64900707b340e18316744686e3eee73111a8f73
      lastState: {}
      name: kube-prometheus-stack
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:12:04Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.185
    podIPs:
    - ip: 10.244.36.185
    qosClass: Burstable
    startTime: "2023-03-24T15:11:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 4e8744dc6e51f16e6387f535263156cd869e2b1e67cae589ceccf5a11084fe83
      checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
      checksum/secret: 55073a9380318970c762e443abea0ef00e556d2269322fc1b8a61b0cf4b1b8db
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container grafana-sc-datasources; cpu, memory, nvidia.com/gpu
        limit for container grafana-sc-datasources; cpu, memory, nvidia.com/gpu request
        for container grafana; cpu, memory, nvidia.com/gpu limit for container grafana;
        cpu, memory, nvidia.com/gpu request for init container init-chown-data; cpu,
        memory, nvidia.com/gpu limit for init container init-chown-data'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:39:39Z"
    generateName: sia-dev-kube-prometheus-stack-grafana-57575897b-
    labels:
      app.kubernetes.io/instance: sia-dev-kube-prometheus-stack
      app.kubernetes.io/name: grafana
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      pod-template-hash: 57575897b
    name: sia-dev-kube-prometheus-stack-grafana-57575897b-lcbq2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sia-dev-kube-prometheus-stack-grafana-57575897b
      uid: 8a6f8a7a-8f2d-4bf4-a897-53a61d3cd1fa
    resourceVersion: "372136052"
    uid: 62941b52-0176-41df-ac7d-a020183e6ab5
  spec:
    automountServiceAccountToken: true
    containers:
    - env:
      - name: METHOD
        value: WATCH
      - name: LABEL
        value: grafana_datasource
      - name: FOLDER
        value: /etc/grafana/provisioning/datasources
      - name: RESOURCE
        value: both
      - name: REQ_USERNAME
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: sia-dev-kube-prometheus-stack-grafana
      - name: REQ_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: sia-dev-kube-prometheus-stack-grafana
      - name: REQ_URL
        value: http://localhost:3000/api/admin/provisioning/datasources/reload
      - name: REQ_METHOD
        value: POST
      image: quay.io/kiwigrid/k8s-sidecar:1.14.2
      imagePullPolicy: IfNotPresent
      name: grafana-sc-datasources
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9htpf
        readOnly: true
    - env:
      - name: GF_SECURITY_ADMIN_USER
        valueFrom:
          secretKeyRef:
            key: admin-user
            name: sia-dev-kube-prometheus-stack-grafana
      - name: GF_SECURITY_ADMIN_PASSWORD
        valueFrom:
          secretKeyRef:
            key: admin-password
            name: sia-dev-kube-prometheus-stack-grafana
      - name: GF_PATHS_DATA
        value: /var/lib/grafana/data
      - name: GF_PATHS_LOGS
        value: /var/log/grafana
      - name: GF_PATHS_PLUGINS
        value: /var/lib/grafana/plugins
      - name: GF_PATHS_PROVISIONING
        value: /etc/grafana/provisioning
      image: grafana/grafana:8.3.3
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 10
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: grafana
      ports:
      - containerPort: 80
        name: http-web
        protocol: TCP
      - containerPort: 3000
        name: grafana
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /api/health
          port: 3000
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        capabilities:
          drop:
          - SYS_ADMIN
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/grafana/grafana.ini
        name: config
        subPath: grafana.ini
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /etc/grafana/provisioning/dashboards/dashboardproviders.yaml
        name: config
        subPath: dashboardproviders.yaml
      - mountPath: /etc/grafana/provisioning/datasources
        name: sc-datasources-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9htpf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - chown
      - -R
      - 472:472
      - /var/lib/grafana
      image: busybox:1.31.1
      imagePullPolicy: IfNotPresent
      name: init-chown-data
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/grafana
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-9htpf
        readOnly: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 472
      runAsGroup: 472
      runAsUser: 472
    serviceAccount: sia-dev-kube-prometheus-stack-grafana
    serviceAccountName: sia-dev-kube-prometheus-stack-grafana
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: sia-dev-kube-prometheus-stack-grafana
      name: config
    - name: storage
      persistentVolumeClaim:
        claimName: sia-dev-kube-prometheus-stack-grafana
    - emptyDir: {}
      name: sc-datasources-volume
    - name: kube-api-access-9htpf
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:43Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-29T06:07:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-29T06:07:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://faece582419fbca29521f159dbdd2423b6660fd0cc6a43e58db8a2e296869111
      image: docker.io/grafana/grafana:8.3.3
      imageID: docker.io/grafana/grafana@sha256:18d94ae734accd66bccf22daed7bdb20c6b99aa0f2c687eea3ce4275fe275062
      lastState: {}
      name: grafana
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:19Z"
    - containerID: containerd://4337934bc006a7514a66cf2208ab758b4ddca411e8976aa9c377c032b74ef4ea
      image: quay.io/kiwigrid/k8s-sidecar:1.14.2
      imageID: quay.io/kiwigrid/k8s-sidecar@sha256:35654389f8a9b7816193a4811cf3ceb6cf309ece8874e84b3d2d8399e618059b
      lastState: {}
      name: grafana-sc-datasources
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:41:01Z"
    hostIP: 10.0.0.23
    initContainerStatuses:
    - containerID: containerd://b07f4dcbd72cebea94b1b6852771cbceb77a0b11d062580cf67df63a3e0649bb
      image: docker.io/library/busybox:1.31.1
      imageID: docker.io/library/busybox@sha256:95cf004f559831017cdf4628aaf1bb30133677be8702a8c5f2994629f637a209
      lastState: {}
      name: init-chown-data
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b07f4dcbd72cebea94b1b6852771cbceb77a0b11d062580cf67df63a3e0649bb
          exitCode: 0
          finishedAt: "2023-03-24T15:40:34Z"
          reason: Completed
          startedAt: "2023-03-24T15:40:34Z"
    phase: Running
    podIP: 10.244.36.200
    podIPs:
    - ip: 10.244.36.200
    qosClass: Burstable
    startTime: "2023-03-24T15:39:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: e0888939579d4610fc937a2c8e9a4dc22c6cfb55ad59813790397014ce3fd450
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory, nvidia.com/gpu
        request for container loki; cpu, memory, nvidia.com/gpu limit for container
        loki'
      prometheus.io/port: http-metrics
      prometheus.io/scrape: "true"
    creationTimestamp: "2023-06-13T04:19:14Z"
    generateName: sia-dev-log-aggregation-system-loki-
    labels:
      app: loki
      app.si-analytics.ai/created-by: haeram.kim1
      app.si-analytics.ai/created-by-serviceaccount: "false"
      controller-revision-hash: sia-dev-log-aggregation-system-loki-694d667c4
      name: loki
      release: sia-dev-log-aggregation-system
      statefulset.kubernetes.io/pod-name: sia-dev-log-aggregation-system-loki-0
    name: sia-dev-log-aggregation-system-loki-0
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: sia-dev-log-aggregation-system-loki
      uid: 4c23d858-5022-4fac-ad81-a761118c2119
    resourceVersion: "439824869"
    uid: 2a94cdd8-2ccb-4563-9e21-dde76efbb8a7
  spec:
    affinity: {}
    containers:
    - args:
      - -config.file=/etc/loki/loki.yaml
      image: grafana/loki:2.2.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: loki
      ports:
      - containerPort: 3100
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: "1"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 500m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/loki
        name: config
      - mountPath: /data
        name: storage
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-jf2bc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: sia-dev-log-aggregation-system-loki-0
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 10001
      runAsGroup: 10001
      runAsNonRoot: true
      runAsUser: 10001
    serviceAccount: sia-dev-log-aggregation-system-loki
    serviceAccountName: sia-dev-log-aggregation-system-loki
    subdomain: sia-dev-log-aggregation-system-loki-headless
    terminationGracePeriodSeconds: 4800
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: storage
      persistentVolumeClaim:
        claimName: storage-sia-dev-log-aggregation-system-loki-0
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-loki
    - name: kube-api-access-jf2bc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T04:19:14Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T04:20:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T04:20:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T04:19:14Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://399dab1763b76254ad59912a4909437a1b589b90ceb6b38cf2f1a1afe0a714e8
      image: docker.io/grafana/loki:2.2.0
      imageID: docker.io/grafana/loki@sha256:83649aa867ffdc353cea17e9465bfc26b1f172c78c19ac906400b5028576c3f3
      lastState: {}
      name: loki
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-06-13T04:19:15Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.45
    podIPs:
    - ip: 10.244.13.45
    qosClass: Burstable
    startTime: "2023-06-13T04:19:14Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:31:52Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-252jc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "438784274"
    uid: 1fe2e2b9-ed15-494e-b25d-50d40363e5e2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-103
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-tmc4k
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-tmc4k
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T01:26:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T01:26:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d0f60f52e77b084397889456778cad97335fb033254848a15496c32a3a86935a
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:31:53Z"
    hostIP: 10.0.0.10
    phase: Running
    podIP: 10.244.20.86
    podIPs:
    - ip: 10.244.20.86
    qosClass: Burstable
    startTime: "2023-03-30T06:31:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:26:41Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-2htjn
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373054961"
    uid: 36e8ed84-d9e2-492f-bae9-caf52ee8ad31
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - v100-301
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-rjb9s
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: v100-301
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-rjb9s
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5aee86b8ca730e784db22a2bde65fc4fe8438d4d48d9fd26f553a570cbb6e0dc
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:26:42Z"
    hostIP: 10.0.0.31
    phase: Running
    podIP: 10.244.28.115
    podIPs:
    - ip: 10.244.28.115
    qosClass: Burstable
    startTime: "2023-03-30T06:26:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-30T06:26:28Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-52w6c
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373054639"
    uid: 3ef188e6-5d9a-47a8-84ef-339ad9ea21f6
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-02
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vcfqh
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: dev-master-02
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-vcfqh
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8e40182d787185877a93c357819d483642c2c524718ea3ea6d1f518e3f34b2cd
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:26:29Z"
    hostIP: 10.0.3.102
    phase: Running
    podIP: 10.244.1.39
    podIPs:
    - ip: 10.244.1.39
    qosClass: Burstable
    startTime: "2023-03-30T06:26:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:26:16Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-5f7ld
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373054473"
    uid: 6e8bedaf-d171-4501-8e60-b92e09c97086
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a40-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-mp77h
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-mp77h
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9f4d31e2bfc743866c35a6617c2791b53497109a59b2ca7c283cce8ba6472e20
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:26:16Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.220
    podIPs:
    - ip: 10.244.13.220
    qosClass: Burstable
    startTime: "2023-03-30T06:26:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:29:01Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-5sfj2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373056591"
    uid: 66d0e15e-d027-4cd1-a2f9-b2387beab961
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-01
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8cjd4
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: dev-master-01
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-8cjd4
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://bf5361d75c1ab8c9354088656a5f735f245e50a456869d1cf862267e969987b2
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:29:02Z"
    hostIP: 10.0.3.101
    phase: Running
    podIP: 10.244.0.148
    podIPs:
    - ip: 10.244.0.148
    qosClass: Burstable
    startTime: "2023-03-30T06:29:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:30:32Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-7k2k2
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373057799"
    uid: d9fd5e29-b174-4d6d-b99f-c7f06e62c7cc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - cpu-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4nhvm
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: cpu-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-4nhvm
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:32Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ccc51fda5aba45e3efe872c3e16b294035e421c7794cc57b232d5fdba00c2e54
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:30:33Z"
    hostIP: 10.0.0.21
    phase: Running
    podIP: 10.244.34.42
    podIPs:
    - ip: 10.244.34.42
    qosClass: Burstable
    startTime: "2023-03-30T06:30:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:29:28Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-7qzh7
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373056929"
    uid: ec4f2e16-930c-4d8a-afc2-3d975fd7a821
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hnh6f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-hnh6f
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:39Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:39Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1fb4bf245fa55c089c2863470bdacc8997a04f1b536f05acbde61d9cc945bada
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:29:29Z"
    hostIP: 10.0.0.12
    phase: Running
    podIP: 10.244.23.69
    podIPs:
    - ip: 10.244.23.69
    qosClass: Burstable
    startTime: "2023-03-30T06:29:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:27:40Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-bjfr8
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "385411235"
    uid: 80c972f4-2f7f-491b-a50a-06ee8bf6d98e
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-202
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-4tqh8
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g2080-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-4tqh8
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:50:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-13T04:50:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e9d6ef81bfafb887bb0593cae3a3f14511a39db45dbc8a6f19795b45dc376042
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState:
        terminated:
          containerID: containerd://38870e10ec080c7b0506db43b0c26f4e488da585172dc130d51f380c20c5fbd0
          exitCode: 255
          finishedAt: "2023-04-13T04:49:21Z"
          reason: Unknown
          startedAt: "2023-03-30T06:27:41Z"
      name: promtail
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:49:47Z"
    hostIP: 10.0.0.26
    phase: Running
    podIP: 10.244.32.3
    podIPs:
    - ip: 10.244.32.3
    qosClass: Burstable
    startTime: "2023-03-30T06:27:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:29:41Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-cln7l
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373057101"
    uid: 48874720-a519-4f49-bb5c-438c500bd0d4
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - cpu-102
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cpwzn
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: cpu-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-cpwzn
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:41Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:53Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:53Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:41Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8d0451fad412466426b2a2a0f8fe92baf8b81d7151865beafedf490f64e932b1
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:29:43Z"
    hostIP: 10.0.0.22
    phase: Running
    podIP: 10.244.35.4
    podIPs:
    - ip: 10.244.35.4
    qosClass: Burstable
    startTime: "2023-03-30T06:29:41Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:30:09Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-dq267
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "439725507"
    uid: 67b83cd1-e1da-438f-913b-1b9f76a4f438
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-102
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-cbcck
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-cbcck
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T01:51:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T01:51:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:09Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4b96db243cc9c78444138d414246e5fdec3fa3bcd9bf5fa75236c824ffa8aeea
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:30:10Z"
    hostIP: 10.0.0.9
    phase: Running
    podIP: 10.244.19.123
    podIPs:
    - ip: 10.244.19.123
    qosClass: Burstable
    startTime: "2023-03-30T06:30:09Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:27:04Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-j2gv5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "439766876"
    uid: 52976d3d-b234-497f-8ffa-0fa4bf1cbc8f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-103
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-g6nd5
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-103
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-g6nd5
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T02:53:34Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T02:53:34Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:27:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://90a678b0f77620a8fbfb3aa349f0cb0785764cb1e63a84374b263ac88f0bffe2
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:27:05Z"
    hostIP: 10.0.0.14
    phase: Running
    podIP: 10.244.37.240
    podIPs:
    - ip: 10.244.37.240
    qosClass: Burstable
    startTime: "2023-03-30T06:27:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:28:39Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-jmwkk
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373056432"
    uid: dfb0575e-8115-482d-9f3a-90826d0fce6a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - dev-master-03
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-8f7fk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: dev-master-03
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-8f7fk
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:59Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:59Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1c4f7e0c8996ab0810b1ee7e52137ca167fc22d3eb4da55f1a82f4964c21284b
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:28:40Z"
    hostIP: 10.0.3.103
    phase: Running
    podIP: 10.244.2.4
    podIPs:
    - ip: 10.244.2.4
    qosClass: Burstable
    startTime: "2023-03-30T06:28:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-30T06:31:07Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-lbxns
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "438394324"
    uid: eea85047-635e-4ac1-8afb-0979eb620f53
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-5n9c7
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-5n9c7
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-11T15:16:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-11T15:16:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://f55a6c5366dc324873fc7e24dc321e85236de2348751e70c78d30c6678e9a789
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:31:08Z"
    hostIP: 10.0.0.8
    phase: Running
    podIP: 10.244.18.39
    podIPs:
    - ip: 10.244.18.39
    qosClass: Burstable
    startTime: "2023-03-30T06:31:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:30:54Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-npkz6
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "415625192"
    uid: 74c57b43-1c24-4c2c-b5b5-9298d1ea046c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-201
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-s9b6b
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-201
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-s9b6b
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:55Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:14:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-17T10:14:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:30:55Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://085bc33b5552164d2e5da1a477aeeda6cb75a945629aae4e962612eb56605fa4
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState:
        terminated:
          containerID: containerd://c8715fae2bd0f5f77097c4cf1f2fd428475a6f0cfdcbe946f2a27259ac37bffc
          exitCode: 255
          finishedAt: "2023-05-17T10:13:03Z"
          reason: Unknown
          startedAt: "2023-05-17T04:20:03Z"
      name: promtail
      ready: true
      restartCount: 10
      started: true
      state:
        running:
          startedAt: "2023-05-17T10:13:24Z"
    hostIP: 10.0.0.19
    phase: Running
    podIP: 10.244.29.50
    podIPs:
    - ip: 10.244.29.50
    qosClass: Burstable
    startTime: "2023-03-30T06:30:55Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:29:14Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-qk6h5
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "421512874"
    uid: 19f5c63f-197e-45c0-9134-302080a449af
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-202
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-bsphs
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-202
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-bsphs
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:08:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-05-24T05:08:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://aaaccd4ca9e75510031c61e92130e80389c7c67a8cebd001a6d0db52d210e6f8
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState:
        terminated:
          containerID: containerd://aeb313df36c8dae32aad4e2121ecd5d0aa57742f781d9d8169db73660bd45f6a
          exitCode: 255
          finishedAt: "2023-05-24T05:07:14Z"
          reason: Unknown
          startedAt: "2023-05-24T03:26:27Z"
      name: promtail
      ready: true
      restartCount: 13
      started: true
      state:
        running:
          startedAt: "2023-05-24T05:07:36Z"
    hostIP: 10.0.0.20
    phase: Running
    podIP: 10.244.30.223
    podIPs:
    - ip: 10.244.30.223
    qosClass: Burstable
    startTime: "2023-03-30T06:29:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-30T06:29:56Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-t5lhc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "391595426"
    uid: d9773b0a-9f88-431a-b7c3-0fbca8c83173
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a100-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-2286q
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a100-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-2286q
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:45:09Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:45:09Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:29:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://017b5668d295e614c7df6bbcbcc6c24af9556af9ac98b16fdf33c784ab6c1fcb
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState:
        terminated:
          containerID: containerd://bb23d234418228a932972bf7cdb7685317502f3315b450711ebe48170d4fc5ca
          exitCode: 255
          finishedAt: "2023-04-20T05:43:47Z"
          reason: Unknown
          startedAt: "2023-03-30T06:29:57Z"
      name: promtail
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-20T05:44:41Z"
    hostIP: 10.0.0.18
    phase: Running
    podIP: 10.244.7.113
    podIPs:
    - ip: 10.244.7.113
    qosClass: Burstable
    startTime: "2023-03-30T06:29:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:28:03Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-tldjc
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "439623645"
    uid: 00ebcec8-37f1-43fe-b3f6-f0faef7d4a9d
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-104
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-vs86x
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-vs86x
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T23:17:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T23:17:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://84f5802b84ab0637e95ad8c7fdfae38bff3c79e4223312eefaab8840e1381966
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:28:04Z"
    hostIP: 10.0.0.15
    phase: Running
    podIP: 10.244.26.3
    podIPs:
    - ip: 10.244.26.3
    qosClass: Burstable
    startTime: "2023-03-30T06:28:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-03-30T06:25:53Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-tms97
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "373054298"
    uid: 95cfce81-7ac6-4cca-b002-e29639bf0772
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - a6000-101
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-77wlj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-77wlj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:25:53Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:13Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:26:13Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:25:53Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5172565819b615cfffa126497bbe935d09247136d37cfa7b0855a862bf08a7ed
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:25:54Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.235
    podIPs:
    - ip: 10.244.36.235
    qosClass: Burstable
    startTime: "2023-03-30T06:25:53Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:28:26Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-wd7tm
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "438751305"
    uid: 92e3d151-a61a-42d4-90c3-473751252486
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g3090-102
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-gn8g9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g3090-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-gn8g9
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T00:38:37Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-12T00:38:37Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:28:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://04dddc731806f860402af673faf70263059479b933bde7014644f54c4619d537
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:28:27Z"
    hostIP: 10.0.0.13
    phase: Running
    podIP: 10.244.24.95
    podIPs:
    - ip: 10.244.24.95
    qosClass: Burstable
    startTime: "2023-03-30T06:28:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:31:29Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-xqmdz
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "439765764"
    uid: 564facf2-bf42-41e4-9338-d2c62b803bb7
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - g2080-102
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-hj88r
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: g2080-102
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-hj88r
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T02:52:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T02:52:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:31:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://29cae7097070a15c1b8bdddab9f293f51568ce4eef08ba15cce2b2ff275cca64
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState:
        terminated:
          containerID: containerd://666b617c3482661c8210a84ac677b4016e269056de27a0c81217c93e25f660eb
          exitCode: 255
          finishedAt: "2023-04-13T04:40:18Z"
          reason: Unknown
          startedAt: "2023-03-30T06:31:31Z"
      name: promtail
      ready: true
      restartCount: 1
      started: true
      state:
        running:
          startedAt: "2023-04-13T04:40:38Z"
    hostIP: 10.0.0.25
    phase: Running
    podIP: 10.244.33.191
    podIPs:
    - ip: 10.244.33.191
    qosClass: Burstable
    startTime: "2023-03-30T06:31:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      checksum/config: 00d03b8b7d803e18fccb0e238f782514e5e328d71e46fc1d8c6d479c044f55bd
      kubectl.kubernetes.io/restartedAt: "2023-03-25T01:14:44+09:00"
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container promtail; nvidia.com/gpu limit for container promtail'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-30T06:25:39Z"
    generateName: sia-dev-log-aggregation-system-promtail-
    labels:
      app.kubernetes.io/instance: sia-dev-log-aggregation-system
      app.kubernetes.io/name: promtail
      app.si-analytics.ai/created-by: daemon-set-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: haeram.kim1
      app.si-analytics.ai/last-edited-by-serviceaccount: "false"
      controller-revision-hash: 8488dfbb45
      pod-template-generation: "6"
    name: sia-dev-log-aggregation-system-promtail-zm5hv
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: sia-dev-log-aggregation-system-promtail
      uid: 08a355a7-e1f0-4828-8311-5052ed61ed98
    resourceVersion: "439713734"
    uid: 0fde3bc0-e8ab-4a7a-baad-901bf4abbc3f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - q5000-104
    containers:
    - args:
      - -config.file=/etc/promtail/promtail.yaml
      env:
      - name: HOSTNAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: docker.io/grafana/promtail:2.2.1
      imagePullPolicy: IfNotPresent
      name: promtail
      ports:
      - containerPort: 3101
        name: http-metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 5
        httpGet:
          path: /ready
          port: http-metrics
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 1500m
          memory: 512Mi
          nvidia.com/gpu: "0"
        requests:
          cpu: 140m
          memory: 256Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/promtail
        name: config
      - mountPath: /run/promtail
        name: run
      - mountPath: /var/log/containers
        name: containers
        readOnly: true
      - mountPath: /var/log/pods
        name: pods
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-klfdj
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: q5000-104
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsGroup: 0
      runAsUser: 0
    serviceAccount: sia-dev-log-aggregation-system-promtail
    serviceAccountName: sia-dev-log-aggregation-system-promtail
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - name: config
      secret:
        defaultMode: 420
        secretName: sia-dev-log-aggregation-system-promtail
    - hostPath:
        path: /run/promtail
        type: ""
      name: run
    - hostPath:
        path: /var/log/containers
        type: ""
      name: containers
    - hostPath:
        path: /var/log/pods
        type: ""
      name: pods
    - name: kube-api-access-klfdj
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:25:40Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T01:33:30Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-06-13T01:33:30Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-30T06:25:40Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b7850068dde1a76deff8368132e343d1f509a6032a69cde74387f8928155b9c8
      image: docker.io/grafana/promtail:2.2.1
      imageID: docker.io/grafana/promtail@sha256:2a93d1cdf6fcbf40f695a9de1e2ee538633986558f73842e3f141d34a9255892
      lastState: {}
      name: promtail
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-30T06:25:40Z"
    hostIP: 10.0.0.11
    phase: Running
    podIP: 10.244.12.57
    podIPs:
    - ip: 10.244.12.57
    qosClass: Burstable
    startTime: "2023-03-30T06:25:40Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container query; nvidia.com/gpu limit for container query'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by-serviceaccount
    creationTimestamp: "2023-03-24T15:39:36Z"
    generateName: tq-query-55bbc7c956-
    labels:
      app.kubernetes.io/component: query
      app.kubernetes.io/instance: sia-d-thanos
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: thanos-query
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      helm.sh/chart: thanos-query-11.5.4
      pod-template-hash: 55bbc7c956
    name: tq-query-55bbc7c956-chqwl
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tq-query-55bbc7c956
      uid: 3f77b77b-1fd8-4b4d-bd21-31f551ec1538
    resourceVersion: "368055048"
    uid: c0afbaae-b5d6-4a76-bcee-91911b89552b
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: query
                app.kubernetes.io/instance: sia-d-thanos
                app.kubernetes.io/name: thanos-query
            namespaces:
            - monitoring
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - args:
      - query
      - --log.level=info
      - --log.format=logfmt
      - --grpc-address=0.0.0.0:10901
      - --http-address=0.0.0.0:10902
      - --query.replica-label=prometheus_replica
      - --endpoint=dns+kps-thanos-discovery.monitoring.svc.dev-cluster.local:10901
      image: docker.io/bitnami/thanos:0.28.0-scratch-r1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: query
      ports:
      - containerPort: 10902
        name: http
        protocol: TCP
      - containerPort: 10901
        name: grpc
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        limits:
          cpu: "4"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 200m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - SYS_ADMIN
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1001
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /-/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-shcjc
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a6000-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: tq-query
    serviceAccountName: tq-query
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-shcjc
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:40:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-03-24T15:39:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d6cf4b2709184143e138dbdc56cf4a78c8fd3199e6764ac35ecec6d99c302d54
      image: docker.io/bitnami/thanos:0.28.0-scratch-r1
      imageID: docker.io/bitnami/thanos@sha256:93120a00aea5d7891da46023efacf77d5fc5a825f153b5773dadc74fbea9f925
      lastState: {}
      name: query
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-03-24T15:39:44Z"
    hostIP: 10.0.0.23
    phase: Running
    podIP: 10.244.36.189
    podIPs:
    - ip: 10.244.36.189
    qosClass: Burstable
    startTime: "2023-03-24T15:39:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: nvidia.com/gpu request
        for container query; nvidia.com/gpu limit for container query'
      policies.kyverno.io/last-applied-patches: |
        metadata-labels.add-created-by-labels.kyverno.io: added /metadata/labels/app.si-analytics.ai~1created-by
    creationTimestamp: "2023-04-20T05:25:54Z"
    generateName: tq-query-55bbc7c956-
    labels:
      app.kubernetes.io/component: query
      app.kubernetes.io/instance: sia-d-thanos
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: thanos-query
      app.si-analytics.ai/created-by: replicaset-controller
      app.si-analytics.ai/created-by-serviceaccount: "true"
      app.si-analytics.ai/last-edited-by: kubernetes-admin
      helm.sh/chart: thanos-query-11.5.4
      pod-template-hash: 55bbc7c956
    name: tq-query-55bbc7c956-ntd7x
    namespace: monitoring
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: tq-query-55bbc7c956
      uid: 3f77b77b-1fd8-4b4d-bd21-31f551ec1538
    resourceVersion: "391583380"
    uid: c6aba0a5-29c7-456b-aed7-39b3f023ccfb
  spec:
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/component: query
                app.kubernetes.io/instance: sia-d-thanos
                app.kubernetes.io/name: thanos-query
            namespaces:
            - monitoring
            topologyKey: kubernetes.io/hostname
          weight: 1
    automountServiceAccountToken: true
    containers:
    - args:
      - query
      - --log.level=info
      - --log.format=logfmt
      - --grpc-address=0.0.0.0:10901
      - --http-address=0.0.0.0:10902
      - --query.replica-label=prometheus_replica
      - --endpoint=dns+kps-thanos-discovery.monitoring.svc.dev-cluster.local:10901
      image: docker.io/bitnami/thanos:0.28.0-scratch-r1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/healthy
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      name: query
      ports:
      - containerPort: 10902
        name: http
        protocol: TCP
      - containerPort: 10901
        name: grpc
        protocol: TCP
      readinessProbe:
        failureThreshold: 6
        httpGet:
          path: /-/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 30
      resources:
        limits:
          cpu: "4"
          memory: 5Gi
          nvidia.com/gpu: "0"
        requests:
          cpu: 200m
          memory: 100Mi
          nvidia.com/gpu: "0"
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - SYS_ADMIN
        readOnlyRootFilesystem: true
        runAsNonRoot: true
        runAsUser: 1001
      startupProbe:
        failureThreshold: 15
        httpGet:
          path: /-/ready
          port: http
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 1
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-api-access-w89ht
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: a40-101
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 1001
    serviceAccount: tq-query
    serviceAccountName: tq-query
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-api-access-w89ht
      projected:
        defaultMode: 420
        sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
        - configMap:
            items:
            - key: ca.crt
              path: ca.crt
            name: kube-root-ca.crt
        - downwardAPI:
            items:
            - fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
              path: namespace
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:25:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:26:35Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:26:35Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2023-04-20T05:25:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://258d915f5df87d18a7bfb953be6a128a683421b83e412da97cc017ed4ae5102b
      image: docker.io/bitnami/thanos:0.28.0-scratch-r1
      imageID: docker.io/bitnami/thanos@sha256:93120a00aea5d7891da46023efacf77d5fc5a825f153b5773dadc74fbea9f925
      lastState: {}
      name: query
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2023-04-20T05:26:00Z"
    hostIP: 10.0.0.30
    phase: Running
    podIP: 10.244.13.91
    podIPs:
    - ip: 10.244.13.91
    qosClass: Burstable
    startTime: "2023-04-20T05:25:54Z"
kind: List
metadata:
  resourceVersion: ""
